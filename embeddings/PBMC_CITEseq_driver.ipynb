{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.utils import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "qualities = np.logspace(-3, 0, 10)\n",
    "replicates = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44babe2e914e42fcb56784e65bb036da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9902ebc31f904c8a9d09580cc3f41fa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n",
      "/home/gokul/.local/lib/python3.10/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n",
      "fractions:   0%|          | 0/10 [00:00<?, ?it/s]\n",
      "/home/gokul/.local/lib/python3.10/site-packages/scvi/model/_utils.py:286: UserWarning: This dataset has some empty cells, this might fail inference.Data should be filtered with `scanpy.pp.filter_cells()`\n",
      "  warnings.warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/home/gokul/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "Monitored metric elbo_validation did not improve in the last 45 records. Best score: 5.442. Signaling Trainer to stop.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "epoch 500/10000 (batch 1000) - train loss: 1.2002, val loss: 0.9835\n",
      "epoch 1000/10000 (batch 2000) - train loss: 0.0505, val loss: 0.0381\n",
      "epoch 1500/10000 (batch 3000) - train loss: 0.0182, val loss: 0.0118\n",
      "epoch 2000/10000 (batch 4000) - train loss: 0.0141, val loss: 0.0115\n",
      "epoch 2500/10000 (batch 5000) - train loss: 0.0072, val loss: 0.0181\n",
      "Early stopping triggered at epoch 2500, batch 5000\n",
      "/home/gokul/.local/lib/python3.10/site-packages/scvi/model/_utils.py:286: UserWarning: This dataset has some empty cells, this might fail inference.Data should be filtered with `scanpy.pp.filter_cells()`\n",
      "  warnings.warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/home/gokul/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "Monitored metric elbo_validation did not improve in the last 45 records. Best score: 4.797. Signaling Trainer to stop.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "/home/gokul/.local/lib/python3.10/site-packages/scvi/model/_utils.py:286: UserWarning: This dataset has some empty cells, this might fail inference.Data should be filtered with `scanpy.pp.filter_cells()`\n",
      "  warnings.warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/home/gokul/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "Monitored metric elbo_validation did not improve in the last 45 records. Best score: 6.000. Signaling Trainer to stop.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "/home/gokul/.local/lib/python3.10/site-packages/scvi/model/_utils.py:286: UserWarning: This dataset has some empty cells, this might fail inference.Data should be filtered with `scanpy.pp.filter_cells()`\n",
      "  warnings.warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/home/gokul/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "Monitored metric elbo_validation did not improve in the last 45 records. Best score: 5.765. Signaling Trainer to stop.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "epoch 36/10000 (batch 1000) - train loss: 6.4883, val loss: 6.4617\n",
      "epoch 72/10000 (batch 2000) - train loss: 5.2991, val loss: 5.2895\n",
      "epoch 108/10000 (batch 3000) - train loss: 3.3599, val loss: 3.3556\n",
      "epoch 143/10000 (batch 4000) - train loss: 1.1230, val loss: 1.1010\n",
      "epoch 179/10000 (batch 5000) - train loss: 0.2409, val loss: 0.2367\n",
      "epoch 215/10000 (batch 6000) - train loss: 0.0819, val loss: 0.0841\n",
      "epoch 250/10000 (batch 7000) - train loss: 0.0435, val loss: 0.0410\n",
      "epoch 286/10000 (batch 8000) - train loss: 0.0277, val loss: 0.0292\n",
      "epoch 322/10000 (batch 9000) - train loss: 0.0190, val loss: 0.0224\n",
      "epoch 358/10000 (batch 10000) - train loss: 0.0173, val loss: 0.0214\n",
      "epoch 393/10000 (batch 11000) - train loss: 0.0155, val loss: 0.0149\n",
      "epoch 429/10000 (batch 12000) - train loss: 0.0120, val loss: 0.0157\n",
      "Early stopping triggered at epoch 429, batch 12000\n",
      "/home/gokul/.local/lib/python3.10/site-packages/scvi/model/_utils.py:286: UserWarning: This dataset has some empty cells, this might fail inference.Data should be filtered with `scanpy.pp.filter_cells()`\n",
      "  warnings.warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/home/gokul/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "Monitored metric elbo_validation did not improve in the last 45 records. Best score: 5.033. Signaling Trainer to stop.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "/home/gokul/.local/lib/python3.10/site-packages/scvi/model/_utils.py:286: UserWarning: This dataset has some empty cells, this might fail inference.Data should be filtered with `scanpy.pp.filter_cells()`\n",
      "  warnings.warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Monitored metric elbo_validation did not improve in the last 45 records. Best score: 5.735. Signaling Trainer to stop.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "/home/gokul/.local/lib/python3.10/site-packages/scvi/model/_utils.py:286: UserWarning: This dataset has some empty cells, this might fail inference.Data should be filtered with `scanpy.pp.filter_cells()`\n",
      "  warnings.warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Monitored metric elbo_validation did not improve in the last 45 records. Best score: 6.038. Signaling Trainer to stop.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "epoch 4/10000 (batch 1000) - train loss: 6.4467, val loss: 6.4240\n",
      "epoch 8/10000 (batch 2000) - train loss: 6.2975, val loss: 6.2970\n",
      "epoch 11/10000 (batch 3000) - train loss: 6.1079, val loss: 6.0894\n",
      "epoch 15/10000 (batch 4000) - train loss: 5.8016, val loss: 5.7997\n",
      "epoch 18/10000 (batch 5000) - train loss: 5.4600, val loss: 5.4283\n",
      "epoch 22/10000 (batch 6000) - train loss: 4.9776, val loss: 4.9739\n",
      "epoch 25/10000 (batch 7000) - train loss: 4.4839, val loss: 4.4375\n",
      "epoch 29/10000 (batch 8000) - train loss: 3.8287, val loss: 3.8221\n",
      "epoch 32/10000 (batch 9000) - train loss: 3.1935, val loss: 3.1343\n",
      "epoch 36/10000 (batch 10000) - train loss: 2.4052, val loss: 2.3954\n",
      "epoch 39/10000 (batch 11000) - train loss: 1.7207, val loss: 1.6606\n",
      "epoch 43/10000 (batch 12000) - train loss: 1.0347, val loss: 1.0242\n",
      "epoch 46/10000 (batch 13000) - train loss: 0.6090, val loss: 0.5768\n",
      "epoch 50/10000 (batch 14000) - train loss: 0.3205, val loss: 0.3156\n",
      "epoch 53/10000 (batch 15000) - train loss: 0.1871, val loss: 0.1777\n",
      "epoch 57/10000 (batch 16000) - train loss: 0.1078, val loss: 0.1062\n",
      "epoch 60/10000 (batch 17000) - train loss: 0.0693, val loss: 0.0672\n",
      "epoch 64/10000 (batch 18000) - train loss: 0.0454, val loss: 0.0460\n",
      "epoch 67/10000 (batch 19000) - train loss: 0.0345, val loss: 0.0334\n",
      "epoch 71/10000 (batch 20000) - train loss: 0.0265, val loss: 0.0268\n",
      "epoch 74/10000 (batch 21000) - train loss: 0.0227, val loss: 0.0226\n",
      "epoch 78/10000 (batch 22000) - train loss: 0.0216, val loss: 0.0202\n",
      "epoch 81/10000 (batch 23000) - train loss: 0.0194, val loss: 0.0188\n",
      "epoch 85/10000 (batch 24000) - train loss: 0.0197, val loss: 0.0197\n",
      "Early stopping triggered at epoch 85, batch 24000\n",
      "/home/gokul/.local/lib/python3.10/site-packages/scvi/model/_utils.py:286: UserWarning: This dataset has some empty cells, this might fail inference.Data should be filtered with `scanpy.pp.filter_cells()`\n",
      "  warnings.warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Monitored metric elbo_validation did not improve in the last 45 records. Best score: 6.071. Signaling Trainer to stop.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "/home/gokul/.local/lib/python3.10/site-packages/scvi/model/_utils.py:286: UserWarning: This dataset has some empty cells, this might fail inference.Data should be filtered with `scanpy.pp.filter_cells()`\n",
      "  warnings.warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Monitored metric elbo_validation did not improve in the last 45 records. Best score: 6.305. Signaling Trainer to stop.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "/home/gokul/.local/lib/python3.10/site-packages/scvi/model/_utils.py:286: UserWarning: This dataset has some empty cells, this might fail inference.Data should be filtered with `scanpy.pp.filter_cells()`\n",
      "  warnings.warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Monitored metric elbo_validation did not improve in the last 45 records. Best score: 6.326. Signaling Trainer to stop.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "epoch 1/10000 (batch 1000) - train loss: 6.5055, val loss: 6.4895\n",
      "epoch 1/10000 (batch 2000) - train loss: 6.4947, val loss: 6.4770\n",
      "epoch 2/10000 (batch 3000) - train loss: 6.4583, val loss: 6.4568\n",
      "epoch 2/10000 (batch 4000) - train loss: 6.4451, val loss: 6.4286\n",
      "epoch 2/10000 (batch 5000) - train loss: 6.4291, val loss: 6.3920\n",
      "epoch 3/10000 (batch 6000) - train loss: 6.3547, val loss: 6.3477\n",
      "epoch 3/10000 (batch 7000) - train loss: 6.3294, val loss: 6.2952\n",
      "epoch 3/10000 (batch 8000) - train loss: 6.3016, val loss: 6.2346\n",
      "epoch 4/10000 (batch 9000) - train loss: 6.1820, val loss: 6.1657\n",
      "epoch 4/10000 (batch 10000) - train loss: 6.1449, val loss: 6.0892\n",
      "epoch 4/10000 (batch 11000) - train loss: 6.1051, val loss: 6.0047\n",
      "epoch 5/10000 (batch 12000) - train loss: 5.9403, val loss: 5.9120\n",
      "epoch 5/10000 (batch 13000) - train loss: 5.8913, val loss: 5.8110\n",
      "epoch 5/10000 (batch 14000) - train loss: 5.8397, val loss: 5.7019\n",
      "epoch 6/10000 (batch 15000) - train loss: 5.6296, val loss: 5.5857\n",
      "epoch 6/10000 (batch 16000) - train loss: 5.5689, val loss: 5.4604\n",
      "epoch 6/10000 (batch 17000) - train loss: 5.5054, val loss: 5.3273\n",
      "epoch 7/10000 (batch 18000) - train loss: 5.2502, val loss: 5.1863\n",
      "epoch 7/10000 (batch 19000) - train loss: 5.1775, val loss: 5.0374\n",
      "epoch 8/10000 (batch 20000) - train loss: 4.8837, val loss: 4.8800\n",
      "epoch 8/10000 (batch 21000) - train loss: 4.8020, val loss: 4.7149\n",
      "epoch 8/10000 (batch 22000) - train loss: 4.7176, val loss: 4.5424\n",
      "epoch 9/10000 (batch 23000) - train loss: 4.3792, val loss: 4.3618\n",
      "epoch 9/10000 (batch 24000) - train loss: 4.2856, val loss: 4.1727\n",
      "epoch 9/10000 (batch 25000) - train loss: 4.1896, val loss: 3.9764\n",
      "epoch 10/10000 (batch 26000) - train loss: 3.8074, val loss: 3.7726\n",
      "epoch 10/10000 (batch 27000) - train loss: 3.7030, val loss: 3.5608\n",
      "epoch 10/10000 (batch 28000) - train loss: 3.5961, val loss: 3.3431\n",
      "epoch 11/10000 (batch 29000) - train loss: 3.1747, val loss: 3.1187\n",
      "epoch 11/10000 (batch 30000) - train loss: 3.0605, val loss: 2.8893\n",
      "epoch 11/10000 (batch 31000) - train loss: 2.9447, val loss: 2.6547\n",
      "epoch 12/10000 (batch 32000) - train loss: 2.4938, val loss: 2.4178\n",
      "epoch 12/10000 (batch 33000) - train loss: 2.3744, val loss: 2.1781\n",
      "epoch 12/10000 (batch 34000) - train loss: 2.2551, val loss: 1.9408\n",
      "epoch 13/10000 (batch 35000) - train loss: 1.7982, val loss: 1.7066\n",
      "epoch 13/10000 (batch 36000) - train loss: 1.6831, val loss: 1.4796\n",
      "epoch 13/10000 (batch 37000) - train loss: 1.5710, val loss: 1.2635\n",
      "epoch 14/10000 (batch 38000) - train loss: 1.1551, val loss: 1.0622\n",
      "epoch 14/10000 (batch 39000) - train loss: 1.0588, val loss: 0.8785\n",
      "epoch 15/10000 (batch 40000) - train loss: 0.7220, val loss: 0.7157\n",
      "epoch 15/10000 (batch 41000) - train loss: 0.6496, val loss: 0.5752\n",
      "epoch 15/10000 (batch 42000) - train loss: 0.5845, val loss: 0.4565\n",
      "epoch 16/10000 (batch 43000) - train loss: 0.3702, val loss: 0.3591\n",
      "epoch 16/10000 (batch 44000) - train loss: 0.3281, val loss: 0.2800\n",
      "epoch 16/10000 (batch 45000) - train loss: 0.2921, val loss: 0.2181\n",
      "epoch 17/10000 (batch 46000) - train loss: 0.1777, val loss: 0.1695\n",
      "epoch 17/10000 (batch 47000) - train loss: 0.1570, val loss: 0.1314\n",
      "epoch 17/10000 (batch 48000) - train loss: 0.1399, val loss: 0.1027\n",
      "epoch 18/10000 (batch 49000) - train loss: 0.0857, val loss: 0.0805\n",
      "epoch 18/10000 (batch 50000) - train loss: 0.0766, val loss: 0.0640\n",
      "epoch 18/10000 (batch 51000) - train loss: 0.0691, val loss: 0.0518\n",
      "epoch 19/10000 (batch 52000) - train loss: 0.0454, val loss: 0.0427\n",
      "epoch 19/10000 (batch 53000) - train loss: 0.0418, val loss: 0.0357\n",
      "epoch 19/10000 (batch 54000) - train loss: 0.0385, val loss: 0.0310\n",
      "epoch 20/10000 (batch 55000) - train loss: 0.0284, val loss: 0.0273\n",
      "epoch 20/10000 (batch 56000) - train loss: 0.0269, val loss: 0.0249\n",
      "epoch 20/10000 (batch 57000) - train loss: 0.0257, val loss: 0.0225\n",
      "epoch 21/10000 (batch 58000) - train loss: 0.0220, val loss: 0.0218\n",
      "epoch 21/10000 (batch 59000) - train loss: 0.0214, val loss: 0.0209\n",
      "epoch 22/10000 (batch 60000) - train loss: 0.0195, val loss: 0.0204\n",
      "epoch 22/10000 (batch 61000) - train loss: 0.0196, val loss: 0.0190\n",
      "epoch 22/10000 (batch 62000) - train loss: 0.0194, val loss: 0.0186\n",
      "epoch 23/10000 (batch 63000) - train loss: 0.0184, val loss: 0.0184\n",
      "epoch 23/10000 (batch 64000) - train loss: 0.0184, val loss: 0.0187\n",
      "Early stopping triggered at epoch 23, batch 64000\n",
      "embedding generation completed! :) \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59bdff4a58d84ff3922c5759f62e9036",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n",
      "/home/gokul/.local/lib/python3.10/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n",
      "fractions:   0%|          | 0/10 [00:00<?, ?it/s]\n",
      "/home/gokul/.local/lib/python3.10/site-packages/scvi/model/_utils.py:286: UserWarning: This dataset has some empty cells, this might fail inference.Data should be filtered with `scanpy.pp.filter_cells()`\n",
      "  warnings.warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/home/gokul/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "Monitored metric elbo_validation did not improve in the last 45 records. Best score: 9.133. Signaling Trainer to stop.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "epoch 500/10000 (batch 1000) - train loss: 1.0928, val loss: 0.8846\n",
      "epoch 1000/10000 (batch 2000) - train loss: 0.0473, val loss: 0.0549\n",
      "epoch 1500/10000 (batch 3000) - train loss: 0.0309, val loss: 0.0539\n",
      "epoch 2000/10000 (batch 4000) - train loss: 0.0244, val loss: 0.0360\n",
      "epoch 2500/10000 (batch 5000) - train loss: 0.0298, val loss: 0.0248\n",
      "epoch 3000/10000 (batch 6000) - train loss: 0.0167, val loss: 0.0438\n",
      "Early stopping triggered at epoch 3000, batch 6000\n",
      "/home/gokul/.local/lib/python3.10/site-packages/scvi/model/_utils.py:286: UserWarning: This dataset has some empty cells, this might fail inference.Data should be filtered with `scanpy.pp.filter_cells()`\n",
      "  warnings.warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/home/gokul/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "Monitored metric elbo_validation did not improve in the last 45 records. Best score: 11.535. Signaling Trainer to stop.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "/home/gokul/.local/lib/python3.10/site-packages/scvi/model/_utils.py:286: UserWarning: This dataset has some empty cells, this might fail inference.Data should be filtered with `scanpy.pp.filter_cells()`\n",
      "  warnings.warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/home/gokul/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "Monitored metric elbo_validation did not improve in the last 45 records. Best score: 14.030. Signaling Trainer to stop.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "/home/gokul/.local/lib/python3.10/site-packages/scvi/model/_utils.py:286: UserWarning: This dataset has some empty cells, this might fail inference.Data should be filtered with `scanpy.pp.filter_cells()`\n",
      "  warnings.warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/home/gokul/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "Monitored metric elbo_validation did not improve in the last 45 records. Best score: 11.439. Signaling Trainer to stop.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "epoch 36/10000 (batch 1000) - train loss: 5.9733, val loss: 5.9175\n",
      "epoch 72/10000 (batch 2000) - train loss: 4.6889, val loss: 4.6777\n",
      "epoch 108/10000 (batch 3000) - train loss: 2.6747, val loss: 2.6679\n",
      "epoch 143/10000 (batch 4000) - train loss: 0.7424, val loss: 0.7243\n",
      "epoch 179/10000 (batch 5000) - train loss: 0.1888, val loss: 0.1829\n",
      "epoch 215/10000 (batch 6000) - train loss: 0.0885, val loss: 0.0792\n",
      "epoch 250/10000 (batch 7000) - train loss: 0.0541, val loss: 0.0559\n",
      "epoch 286/10000 (batch 8000) - train loss: 0.0419, val loss: 0.0399\n",
      "epoch 322/10000 (batch 9000) - train loss: 0.0382, val loss: 0.0328\n",
      "epoch 358/10000 (batch 10000) - train loss: 0.0339, val loss: 0.0315\n",
      "epoch 393/10000 (batch 11000) - train loss: 0.0313, val loss: 0.0301\n",
      "epoch 429/10000 (batch 12000) - train loss: 0.0259, val loss: 0.0264\n",
      "epoch 465/10000 (batch 13000) - train loss: 0.0226, val loss: 0.0175\n",
      "epoch 500/10000 (batch 14000) - train loss: 0.0214, val loss: 0.0199\n",
      "Early stopping triggered at epoch 500, batch 14000\n",
      "/home/gokul/.local/lib/python3.10/site-packages/scvi/model/_utils.py:286: UserWarning: This dataset has some empty cells, this might fail inference.Data should be filtered with `scanpy.pp.filter_cells()`\n",
      "  warnings.warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/home/gokul/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "Monitored metric elbo_validation did not improve in the last 45 records. Best score: 12.726. Signaling Trainer to stop.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "/home/gokul/.local/lib/python3.10/site-packages/scvi/model/_utils.py:286: UserWarning: This dataset has some empty cells, this might fail inference.Data should be filtered with `scanpy.pp.filter_cells()`\n",
      "  warnings.warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Monitored metric elbo_validation did not improve in the last 45 records. Best score: 12.008. Signaling Trainer to stop.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "/home/gokul/.local/lib/python3.10/site-packages/scvi/model/_utils.py:286: UserWarning: This dataset has some empty cells, this might fail inference.Data should be filtered with `scanpy.pp.filter_cells()`\n",
      "  warnings.warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Monitored metric elbo_validation did not improve in the last 45 records. Best score: 12.575. Signaling Trainer to stop.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "epoch 4/10000 (batch 1000) - train loss: 8.0495, val loss: 8.1161\n",
      "epoch 8/10000 (batch 2000) - train loss: 7.9906, val loss: 7.9899\n",
      "epoch 11/10000 (batch 3000) - train loss: 7.8021, val loss: 7.7838\n",
      "epoch 15/10000 (batch 4000) - train loss: 7.4990, val loss: 7.4974\n",
      "epoch 18/10000 (batch 5000) - train loss: 7.1609, val loss: 7.1296\n",
      "epoch 22/10000 (batch 6000) - train loss: 6.6845, val loss: 6.6806\n",
      "epoch 25/10000 (batch 7000) - train loss: 6.1960, val loss: 6.1502\n",
      "epoch 29/10000 (batch 8000) - train loss: 5.5452, val loss: 5.5376\n",
      "epoch 32/10000 (batch 9000) - train loss: 4.9040, val loss: 4.8429\n",
      "epoch 36/10000 (batch 10000) - train loss: 4.0780, val loss: 4.0679\n",
      "epoch 39/10000 (batch 11000) - train loss: 3.2975, val loss: 3.2220\n",
      "epoch 43/10000 (batch 12000) - train loss: 2.3531, val loss: 2.3364\n",
      "epoch 46/10000 (batch 13000) - train loss: 1.5668, val loss: 1.4971\n",
      "epoch 50/10000 (batch 14000) - train loss: 0.8499, val loss: 0.8399\n",
      "epoch 53/10000 (batch 15000) - train loss: 0.4708, val loss: 0.4412\n",
      "epoch 57/10000 (batch 16000) - train loss: 0.2461, val loss: 0.2389\n",
      "epoch 60/10000 (batch 17000) - train loss: 0.1487, val loss: 0.1391\n",
      "epoch 64/10000 (batch 18000) - train loss: 0.0952, val loss: 0.0896\n",
      "epoch 67/10000 (batch 19000) - train loss: 0.0679, val loss: 0.0652\n",
      "epoch 71/10000 (batch 20000) - train loss: 0.0526, val loss: 0.0486\n",
      "epoch 74/10000 (batch 21000) - train loss: 0.0448, val loss: 0.0429\n",
      "epoch 78/10000 (batch 22000) - train loss: 0.0396, val loss: 0.0374\n",
      "epoch 81/10000 (batch 23000) - train loss: 0.0365, val loss: 0.0350\n",
      "epoch 85/10000 (batch 24000) - train loss: 0.0369, val loss: 0.0326\n",
      "epoch 88/10000 (batch 25000) - train loss: 0.0343, val loss: 0.0339\n",
      "Early stopping triggered at epoch 88, batch 25000\n",
      "/home/gokul/.local/lib/python3.10/site-packages/scvi/model/_utils.py:286: UserWarning: This dataset has some empty cells, this might fail inference.Data should be filtered with `scanpy.pp.filter_cells()`\n",
      "  warnings.warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Monitored metric elbo_validation did not improve in the last 45 records. Best score: 12.693. Signaling Trainer to stop.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "/home/gokul/.local/lib/python3.10/site-packages/scvi/model/_utils.py:286: UserWarning: This dataset has some empty cells, this might fail inference.Data should be filtered with `scanpy.pp.filter_cells()`\n",
      "  warnings.warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Monitored metric elbo_validation did not improve in the last 45 records. Best score: 12.530. Signaling Trainer to stop.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "/home/gokul/.local/lib/python3.10/site-packages/scvi/model/_utils.py:286: UserWarning: This dataset has some empty cells, this might fail inference.Data should be filtered with `scanpy.pp.filter_cells()`\n",
      "  warnings.warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Monitored metric elbo_validation did not improve in the last 45 records. Best score: 13.146. Signaling Trainer to stop.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "epoch 1/10000 (batch 1000) - train loss: 6.1188, val loss: 6.0756\n",
      "epoch 1/10000 (batch 2000) - train loss: 6.0945, val loss: 6.0634\n",
      "epoch 2/10000 (batch 3000) - train loss: 6.0449, val loss: 6.0433\n",
      "epoch 2/10000 (batch 4000) - train loss: 6.0317, val loss: 6.0150\n",
      "epoch 2/10000 (batch 5000) - train loss: 6.0159, val loss: 5.9789\n",
      "epoch 3/10000 (batch 6000) - train loss: 5.9418, val loss: 5.9349\n",
      "epoch 3/10000 (batch 7000) - train loss: 5.9167, val loss: 5.8825\n",
      "epoch 3/10000 (batch 8000) - train loss: 5.8891, val loss: 5.8226\n",
      "epoch 4/10000 (batch 9000) - train loss: 5.7703, val loss: 5.7545\n",
      "epoch 4/10000 (batch 10000) - train loss: 5.7334, val loss: 5.6782\n",
      "epoch 4/10000 (batch 11000) - train loss: 5.6939, val loss: 5.5940\n",
      "epoch 5/10000 (batch 12000) - train loss: 5.5299, val loss: 5.5018\n",
      "epoch 5/10000 (batch 13000) - train loss: 5.4811, val loss: 5.4010\n",
      "epoch 5/10000 (batch 14000) - train loss: 5.4297, val loss: 5.2930\n",
      "epoch 6/10000 (batch 15000) - train loss: 5.2207, val loss: 5.1765\n",
      "epoch 6/10000 (batch 16000) - train loss: 5.1602, val loss: 5.0519\n",
      "epoch 6/10000 (batch 17000) - train loss: 5.0969, val loss: 4.9196\n",
      "epoch 7/10000 (batch 18000) - train loss: 4.8427, val loss: 4.7791\n",
      "epoch 7/10000 (batch 19000) - train loss: 4.7703, val loss: 4.6307\n",
      "epoch 8/10000 (batch 20000) - train loss: 4.4779, val loss: 4.4743\n",
      "epoch 8/10000 (batch 21000) - train loss: 4.3962, val loss: 4.3096\n",
      "epoch 8/10000 (batch 22000) - train loss: 4.3121, val loss: 4.1376\n",
      "epoch 9/10000 (batch 23000) - train loss: 3.9756, val loss: 3.9578\n",
      "epoch 9/10000 (batch 24000) - train loss: 3.8824, val loss: 3.7700\n",
      "epoch 9/10000 (batch 25000) - train loss: 3.7872, val loss: 3.5754\n",
      "epoch 10/10000 (batch 26000) - train loss: 3.4089, val loss: 3.3739\n",
      "epoch 10/10000 (batch 27000) - train loss: 3.3056, val loss: 3.1654\n",
      "epoch 10/10000 (batch 28000) - train loss: 3.2002, val loss: 2.9518\n",
      "epoch 11/10000 (batch 29000) - train loss: 2.7862, val loss: 2.7324\n",
      "epoch 11/10000 (batch 30000) - train loss: 2.6754, val loss: 2.5088\n",
      "epoch 11/10000 (batch 31000) - train loss: 2.5631, val loss: 2.2832\n",
      "epoch 12/10000 (batch 32000) - train loss: 2.1285, val loss: 2.0562\n",
      "epoch 12/10000 (batch 33000) - train loss: 2.0156, val loss: 1.8312\n",
      "epoch 12/10000 (batch 34000) - train loss: 1.9035, val loss: 1.6100\n",
      "epoch 13/10000 (batch 35000) - train loss: 1.4797, val loss: 1.3958\n",
      "epoch 13/10000 (batch 36000) - train loss: 1.3756, val loss: 1.1939\n",
      "epoch 13/10000 (batch 37000) - train loss: 1.2762, val loss: 1.0056\n",
      "epoch 14/10000 (batch 38000) - train loss: 0.9132, val loss: 0.8348\n",
      "epoch 14/10000 (batch 39000) - train loss: 0.8331, val loss: 0.6842\n",
      "epoch 15/10000 (batch 40000) - train loss: 0.5593, val loss: 0.5539\n",
      "epoch 15/10000 (batch 41000) - train loss: 0.5027, val loss: 0.4439\n",
      "epoch 15/10000 (batch 42000) - train loss: 0.4522, val loss: 0.3537\n",
      "epoch 16/10000 (batch 43000) - train loss: 0.2885, val loss: 0.2807\n",
      "epoch 16/10000 (batch 44000) - train loss: 0.2576, val loss: 0.2228\n",
      "epoch 16/10000 (batch 45000) - train loss: 0.2313, val loss: 0.1769\n",
      "epoch 17/10000 (batch 46000) - train loss: 0.1473, val loss: 0.1419\n",
      "epoch 17/10000 (batch 47000) - train loss: 0.1327, val loss: 0.1149\n",
      "epoch 17/10000 (batch 48000) - train loss: 0.1205, val loss: 0.0939\n",
      "epoch 18/10000 (batch 49000) - train loss: 0.0816, val loss: 0.0784\n",
      "epoch 18/10000 (batch 50000) - train loss: 0.0754, val loss: 0.0671\n",
      "epoch 18/10000 (batch 51000) - train loss: 0.0700, val loss: 0.0581\n",
      "epoch 19/10000 (batch 52000) - train loss: 0.0533, val loss: 0.0517\n",
      "epoch 19/10000 (batch 53000) - train loss: 0.0511, val loss: 0.0477\n",
      "epoch 19/10000 (batch 54000) - train loss: 0.0491, val loss: 0.0437\n",
      "epoch 20/10000 (batch 55000) - train loss: 0.0428, val loss: 0.0413\n",
      "epoch 20/10000 (batch 56000) - train loss: 0.0418, val loss: 0.0400\n",
      "epoch 20/10000 (batch 57000) - train loss: 0.0410, val loss: 0.0384\n",
      "epoch 21/10000 (batch 58000) - train loss: 0.0388, val loss: 0.0380\n",
      "epoch 21/10000 (batch 59000) - train loss: 0.0382, val loss: 0.0371\n",
      "epoch 22/10000 (batch 60000) - train loss: 0.0374, val loss: 0.0371\n",
      "Early stopping triggered at epoch 22, batch 60000\n",
      "embedding generation completed! :) \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1136726daa4439aac6074455c4148ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n",
      "/home/gokul/.local/lib/python3.10/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n",
      "fractions:   0%|          | 0/10 [00:00<?, ?it/s]\n",
      "/home/gokul/.local/lib/python3.10/site-packages/scvi/model/_utils.py:286: UserWarning: This dataset has some empty cells, this might fail inference.Data should be filtered with `scanpy.pp.filter_cells()`\n",
      "  warnings.warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/home/gokul/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "Monitored metric elbo_validation did not improve in the last 45 records. Best score: 16.976. Signaling Trainer to stop.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "epoch 500/10000 (batch 1000) - train loss: 1.8118, val loss: 1.5856\n",
      "epoch 1000/10000 (batch 2000) - train loss: 0.0867, val loss: 0.0740\n",
      "epoch 1500/10000 (batch 3000) - train loss: 0.0712, val loss: 0.0349\n",
      "epoch 2000/10000 (batch 4000) - train loss: 0.0518, val loss: 0.0414\n",
      "Early stopping triggered at epoch 2000, batch 4000\n",
      "/home/gokul/.local/lib/python3.10/site-packages/scvi/model/_utils.py:286: UserWarning: This dataset has some empty cells, this might fail inference.Data should be filtered with `scanpy.pp.filter_cells()`\n",
      "  warnings.warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/home/gokul/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "Monitored metric elbo_validation did not improve in the last 45 records. Best score: 19.029. Signaling Trainer to stop.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "/home/gokul/.local/lib/python3.10/site-packages/scvi/model/_utils.py:286: UserWarning: This dataset has some empty cells, this might fail inference.Data should be filtered with `scanpy.pp.filter_cells()`\n",
      "  warnings.warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/home/gokul/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "Monitored metric elbo_validation did not improve in the last 45 records. Best score: 20.231. Signaling Trainer to stop.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "/home/gokul/.local/lib/python3.10/site-packages/scvi/model/_utils.py:286: UserWarning: This dataset has some empty cells, this might fail inference.Data should be filtered with `scanpy.pp.filter_cells()`\n",
      "  warnings.warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/home/gokul/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "Monitored metric elbo_validation did not improve in the last 45 records. Best score: 21.806. Signaling Trainer to stop.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "epoch 36/10000 (batch 1000) - train loss: 6.4149, val loss: 6.3829\n",
      "epoch 72/10000 (batch 2000) - train loss: 5.1500, val loss: 5.1380\n",
      "epoch 108/10000 (batch 3000) - train loss: 3.1156, val loss: 3.1155\n",
      "epoch 143/10000 (batch 4000) - train loss: 0.9592, val loss: 0.9448\n",
      "epoch 179/10000 (batch 5000) - train loss: 0.2358, val loss: 0.2416\n",
      "epoch 215/10000 (batch 6000) - train loss: 0.1077, val loss: 0.1170\n",
      "epoch 250/10000 (batch 7000) - train loss: 0.0785, val loss: 0.0835\n",
      "epoch 286/10000 (batch 8000) - train loss: 0.0595, val loss: 0.0672\n",
      "epoch 322/10000 (batch 9000) - train loss: 0.0592, val loss: 0.0596\n",
      "epoch 358/10000 (batch 10000) - train loss: 0.0446, val loss: 0.0535\n",
      "epoch 393/10000 (batch 11000) - train loss: 0.0458, val loss: 0.0466\n",
      "epoch 429/10000 (batch 12000) - train loss: 0.0408, val loss: 0.0462\n",
      "epoch 465/10000 (batch 13000) - train loss: 0.0362, val loss: 0.0398\n",
      "epoch 500/10000 (batch 14000) - train loss: 0.0340, val loss: 0.0426\n",
      "Early stopping triggered at epoch 500, batch 14000\n",
      "/home/gokul/.local/lib/python3.10/site-packages/scvi/model/_utils.py:286: UserWarning: This dataset has some empty cells, this might fail inference.Data should be filtered with `scanpy.pp.filter_cells()`\n",
      "  warnings.warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/home/gokul/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "Monitored metric elbo_validation did not improve in the last 45 records. Best score: 18.440. Signaling Trainer to stop.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "/home/gokul/.local/lib/python3.10/site-packages/scvi/model/_utils.py:286: UserWarning: This dataset has some empty cells, this might fail inference.Data should be filtered with `scanpy.pp.filter_cells()`\n",
      "  warnings.warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Monitored metric elbo_validation did not improve in the last 45 records. Best score: 20.071. Signaling Trainer to stop.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "/home/gokul/.local/lib/python3.10/site-packages/scvi/model/_utils.py:286: UserWarning: This dataset has some empty cells, this might fail inference.Data should be filtered with `scanpy.pp.filter_cells()`\n",
      "  warnings.warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Monitored metric elbo_validation did not improve in the last 45 records. Best score: 21.086. Signaling Trainer to stop.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "epoch 4/10000 (batch 1000) - train loss: 7.4567, val loss: 7.4886\n",
      "epoch 8/10000 (batch 2000) - train loss: 7.3683, val loss: 7.3683\n",
      "epoch 11/10000 (batch 3000) - train loss: 7.1868, val loss: 7.1691\n",
      "epoch 15/10000 (batch 4000) - train loss: 6.8939, val loss: 6.8922\n",
      "epoch 18/10000 (batch 5000) - train loss: 6.5672, val loss: 6.5367\n",
      "epoch 22/10000 (batch 6000) - train loss: 6.1056, val loss: 6.1018\n",
      "epoch 25/10000 (batch 7000) - train loss: 5.6321, val loss: 5.5877\n",
      "epoch 29/10000 (batch 8000) - train loss: 5.0014, val loss: 4.9927\n",
      "epoch 32/10000 (batch 9000) - train loss: 4.3781, val loss: 4.3189\n",
      "epoch 36/10000 (batch 10000) - train loss: 3.5807, val loss: 3.5694\n",
      "epoch 39/10000 (batch 11000) - train loss: 2.8328, val loss: 2.7627\n",
      "epoch 43/10000 (batch 12000) - train loss: 1.9555, val loss: 1.9428\n",
      "epoch 46/10000 (batch 13000) - train loss: 1.2689, val loss: 1.2120\n",
      "epoch 50/10000 (batch 14000) - train loss: 0.6940, val loss: 0.6855\n",
      "epoch 53/10000 (batch 15000) - train loss: 0.4015, val loss: 0.3832\n",
      "epoch 57/10000 (batch 16000) - train loss: 0.2263, val loss: 0.2279\n",
      "epoch 60/10000 (batch 17000) - train loss: 0.1520, val loss: 0.1472\n",
      "epoch 64/10000 (batch 18000) - train loss: 0.1063, val loss: 0.1065\n",
      "epoch 67/10000 (batch 19000) - train loss: 0.0865, val loss: 0.0853\n",
      "epoch 71/10000 (batch 20000) - train loss: 0.0710, val loss: 0.0722\n",
      "epoch 74/10000 (batch 21000) - train loss: 0.0675, val loss: 0.0661\n",
      "epoch 78/10000 (batch 22000) - train loss: 0.0634, val loss: 0.0640\n",
      "epoch 81/10000 (batch 23000) - train loss: 0.0609, val loss: 0.0613\n",
      "epoch 85/10000 (batch 24000) - train loss: 0.0603, val loss: 0.0602\n",
      "epoch 88/10000 (batch 25000) - train loss: 0.0569, val loss: 0.0588\n",
      "epoch 92/10000 (batch 26000) - train loss: 0.0551, val loss: 0.0545\n",
      "epoch 95/10000 (batch 27000) - train loss: 0.0542, val loss: 0.0534\n",
      "epoch 99/10000 (batch 28000) - train loss: 0.0522, val loss: 0.0526\n",
      "epoch 102/10000 (batch 29000) - train loss: 0.0520, val loss: 0.0508\n",
      "epoch 106/10000 (batch 30000) - train loss: 0.0477, val loss: 0.0483\n",
      "epoch 109/10000 (batch 31000) - train loss: 0.0451, val loss: 0.0442\n",
      "epoch 113/10000 (batch 32000) - train loss: 0.0408, val loss: 0.0415\n",
      "epoch 116/10000 (batch 33000) - train loss: 0.0400, val loss: 0.0384\n",
      "epoch 120/10000 (batch 34000) - train loss: 0.0388, val loss: 0.0391\n",
      "Early stopping triggered at epoch 120, batch 34000\n",
      "/home/gokul/.local/lib/python3.10/site-packages/scvi/model/_utils.py:286: UserWarning: This dataset has some empty cells, this might fail inference.Data should be filtered with `scanpy.pp.filter_cells()`\n",
      "  warnings.warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Monitored metric elbo_validation did not improve in the last 45 records. Best score: 21.442. Signaling Trainer to stop.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "/home/gokul/.local/lib/python3.10/site-packages/scvi/model/_utils.py:286: UserWarning: This dataset has some empty cells, this might fail inference.Data should be filtered with `scanpy.pp.filter_cells()`\n",
      "  warnings.warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Monitored metric elbo_validation did not improve in the last 45 records. Best score: 21.840. Signaling Trainer to stop.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "/home/gokul/.local/lib/python3.10/site-packages/scvi/model/_utils.py:286: UserWarning: This dataset has some empty cells, this might fail inference.Data should be filtered with `scanpy.pp.filter_cells()`\n",
      "  warnings.warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Monitored metric elbo_validation did not improve in the last 45 records. Best score: 22.865. Signaling Trainer to stop.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "epoch 1/10000 (batch 1000) - train loss: 7.0756, val loss: 7.0914\n",
      "epoch 1/10000 (batch 2000) - train loss: 7.0809, val loss: 7.0796\n",
      "epoch 2/10000 (batch 3000) - train loss: 7.0611, val loss: 7.0590\n",
      "epoch 2/10000 (batch 4000) - train loss: 7.0480, val loss: 7.0315\n",
      "epoch 2/10000 (batch 5000) - train loss: 7.0324, val loss: 6.9960\n",
      "epoch 3/10000 (batch 6000) - train loss: 6.9592, val loss: 6.9521\n",
      "epoch 3/10000 (batch 7000) - train loss: 6.9345, val loss: 6.9008\n",
      "epoch 3/10000 (batch 8000) - train loss: 6.9071, val loss: 6.8411\n",
      "epoch 4/10000 (batch 9000) - train loss: 6.7897, val loss: 6.7741\n",
      "epoch 4/10000 (batch 10000) - train loss: 6.7533, val loss: 6.6987\n",
      "epoch 4/10000 (batch 11000) - train loss: 6.7143, val loss: 6.6157\n",
      "epoch 5/10000 (batch 12000) - train loss: 6.5525, val loss: 6.5246\n",
      "epoch 5/10000 (batch 13000) - train loss: 6.5044, val loss: 6.4254\n",
      "epoch 5/10000 (batch 14000) - train loss: 6.4537, val loss: 6.3186\n",
      "epoch 6/10000 (batch 15000) - train loss: 6.2476, val loss: 6.2042\n",
      "epoch 6/10000 (batch 16000) - train loss: 6.1879, val loss: 6.0811\n",
      "epoch 6/10000 (batch 17000) - train loss: 6.1255, val loss: 5.9507\n",
      "epoch 7/10000 (batch 18000) - train loss: 5.8749, val loss: 5.8125\n",
      "epoch 7/10000 (batch 19000) - train loss: 5.8036, val loss: 5.6658\n",
      "epoch 8/10000 (batch 20000) - train loss: 5.5142, val loss: 5.5114\n",
      "epoch 8/10000 (batch 21000) - train loss: 5.4344, val loss: 5.3491\n",
      "epoch 8/10000 (batch 22000) - train loss: 5.3515, val loss: 5.1795\n",
      "epoch 9/10000 (batch 23000) - train loss: 5.0187, val loss: 5.0012\n",
      "epoch 9/10000 (batch 24000) - train loss: 4.9266, val loss: 4.8153\n",
      "epoch 9/10000 (batch 25000) - train loss: 4.8319, val loss: 4.6216\n",
      "epoch 10/10000 (batch 26000) - train loss: 4.4553, val loss: 4.4202\n",
      "epoch 10/10000 (batch 27000) - train loss: 4.3517, val loss: 4.2108\n",
      "epoch 10/10000 (batch 28000) - train loss: 4.2456, val loss: 3.9946\n",
      "epoch 11/10000 (batch 29000) - train loss: 3.8262, val loss: 3.7713\n",
      "epoch 11/10000 (batch 30000) - train loss: 3.7122, val loss: 3.5404\n",
      "epoch 11/10000 (batch 31000) - train loss: 3.5957, val loss: 3.3034\n",
      "epoch 12/10000 (batch 32000) - train loss: 3.1387, val loss: 3.0610\n",
      "epoch 12/10000 (batch 33000) - train loss: 3.0158, val loss: 2.8130\n",
      "epoch 12/10000 (batch 34000) - train loss: 2.8917, val loss: 2.5624\n",
      "epoch 13/10000 (batch 35000) - train loss: 2.4094, val loss: 2.3102\n",
      "epoch 13/10000 (batch 36000) - train loss: 2.2835, val loss: 2.0586\n",
      "epoch 13/10000 (batch 37000) - train loss: 2.1582, val loss: 1.8104\n",
      "epoch 14/10000 (batch 38000) - train loss: 1.6822, val loss: 1.5702\n",
      "epoch 14/10000 (batch 39000) - train loss: 1.5646, val loss: 1.3412\n",
      "epoch 15/10000 (batch 40000) - train loss: 1.1350, val loss: 1.1288\n",
      "epoch 15/10000 (batch 41000) - train loss: 1.0380, val loss: 0.9348\n",
      "epoch 15/10000 (batch 42000) - train loss: 0.9463, val loss: 0.7631\n",
      "epoch 16/10000 (batch 43000) - train loss: 0.6329, val loss: 0.6167\n",
      "epoch 16/10000 (batch 44000) - train loss: 0.5688, val loss: 0.4937\n",
      "epoch 16/10000 (batch 45000) - train loss: 0.5118, val loss: 0.3939\n",
      "epoch 17/10000 (batch 46000) - train loss: 0.3278, val loss: 0.3143\n",
      "epoch 17/10000 (batch 47000) - train loss: 0.2942, val loss: 0.2512\n",
      "epoch 17/10000 (batch 48000) - train loss: 0.2654, val loss: 0.2027\n",
      "epoch 18/10000 (batch 49000) - train loss: 0.1746, val loss: 0.1663\n",
      "epoch 18/10000 (batch 50000) - train loss: 0.1594, val loss: 0.1379\n",
      "epoch 18/10000 (batch 51000) - train loss: 0.1465, val loss: 0.1178\n",
      "epoch 19/10000 (batch 52000) - train loss: 0.1071, val loss: 0.1024\n",
      "epoch 19/10000 (batch 53000) - train loss: 0.1003, val loss: 0.0911\n",
      "epoch 19/10000 (batch 54000) - train loss: 0.0952, val loss: 0.0826\n",
      "epoch 20/10000 (batch 55000) - train loss: 0.0778, val loss: 0.0776\n",
      "epoch 20/10000 (batch 56000) - train loss: 0.0761, val loss: 0.0720\n",
      "epoch 20/10000 (batch 57000) - train loss: 0.0744, val loss: 0.0693\n",
      "epoch 21/10000 (batch 58000) - train loss: 0.0674, val loss: 0.0671\n",
      "epoch 21/10000 (batch 59000) - train loss: 0.0670, val loss: 0.0655\n",
      "epoch 22/10000 (batch 60000) - train loss: 0.0641, val loss: 0.0647\n",
      "epoch 22/10000 (batch 61000) - train loss: 0.0644, val loss: 0.0646\n",
      "epoch 22/10000 (batch 62000) - train loss: 0.0638, val loss: 0.0636\n",
      "epoch 23/10000 (batch 63000) - train loss: 0.0610, val loss: 0.0634\n",
      "epoch 23/10000 (batch 64000) - train loss: 0.0623, val loss: 0.0618\n",
      "epoch 23/10000 (batch 65000) - train loss: 0.0623, val loss: 0.0616\n",
      "epoch 24/10000 (batch 66000) - train loss: 0.0605, val loss: 0.0611\n",
      "epoch 24/10000 (batch 67000) - train loss: 0.0604, val loss: 0.0615\n",
      "Early stopping triggered at epoch 24, batch 67000\n",
      "embedding generation completed! :) \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "962db834fc70438981daeacc59bb1eac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n",
      "/home/gokul/.local/lib/python3.10/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n",
      "fractions:   0%|          | 0/10 [00:00<?, ?it/s]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/home/gokul/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "Monitored metric elbo_validation did not improve in the last 45 records. Best score: 28.496. Signaling Trainer to stop.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "epoch 500/10000 (batch 1000) - train loss: 2.3208, val loss: 2.0986\n",
      "epoch 1000/10000 (batch 2000) - train loss: 0.1125, val loss: 0.0832\n",
      "epoch 1500/10000 (batch 3000) - train loss: 0.1002, val loss: 0.0840\n",
      "Early stopping triggered at epoch 1500, batch 3000\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/home/gokul/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "Monitored metric elbo_validation did not improve in the last 45 records. Best score: 34.171. Signaling Trainer to stop.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/home/gokul/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "Monitored metric elbo_validation did not improve in the last 45 records. Best score: 26.976. Signaling Trainer to stop.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "/home/gokul/.local/lib/python3.10/site-packages/scvi/model/_utils.py:286: UserWarning: This dataset has some empty cells, this might fail inference.Data should be filtered with `scanpy.pp.filter_cells()`\n",
      "  warnings.warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/home/gokul/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "Monitored metric elbo_validation did not improve in the last 45 records. Best score: 32.609. Signaling Trainer to stop.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "epoch 36/10000 (batch 1000) - train loss: 5.9508, val loss: 5.8976\n",
      "epoch 72/10000 (batch 2000) - train loss: 4.6643, val loss: 4.6537\n",
      "epoch 108/10000 (batch 3000) - train loss: 2.6401, val loss: 2.6381\n",
      "epoch 143/10000 (batch 4000) - train loss: 0.7587, val loss: 0.7388\n",
      "epoch 179/10000 (batch 5000) - train loss: 0.2491, val loss: 0.2290\n",
      "epoch 215/10000 (batch 6000) - train loss: 0.1463, val loss: 0.1468\n",
      "epoch 250/10000 (batch 7000) - train loss: 0.1120, val loss: 0.1073\n",
      "epoch 286/10000 (batch 8000) - train loss: 0.1015, val loss: 0.0970\n",
      "epoch 322/10000 (batch 9000) - train loss: 0.0981, val loss: 0.0900\n",
      "epoch 358/10000 (batch 10000) - train loss: 0.0868, val loss: 0.0832\n",
      "epoch 393/10000 (batch 11000) - train loss: 0.0772, val loss: 0.0727\n",
      "epoch 429/10000 (batch 12000) - train loss: 0.0698, val loss: 0.0620\n",
      "epoch 465/10000 (batch 13000) - train loss: 0.0631, val loss: 0.0594\n",
      "epoch 500/10000 (batch 14000) - train loss: 0.0656, val loss: 0.0638\n",
      "Early stopping triggered at epoch 500, batch 14000\n",
      "/home/gokul/.local/lib/python3.10/site-packages/scvi/model/_utils.py:286: UserWarning: This dataset has some empty cells, this might fail inference.Data should be filtered with `scanpy.pp.filter_cells()`\n",
      "  warnings.warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/home/gokul/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "Monitored metric elbo_validation did not improve in the last 45 records. Best score: 31.549. Signaling Trainer to stop.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "/home/gokul/.local/lib/python3.10/site-packages/scvi/model/_utils.py:286: UserWarning: This dataset has some empty cells, this might fail inference.Data should be filtered with `scanpy.pp.filter_cells()`\n",
      "  warnings.warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Monitored metric elbo_validation did not improve in the last 45 records. Best score: 32.837. Signaling Trainer to stop.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "/home/gokul/.local/lib/python3.10/site-packages/scvi/model/_utils.py:286: UserWarning: This dataset has some empty cells, this might fail inference.Data should be filtered with `scanpy.pp.filter_cells()`\n",
      "  warnings.warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Monitored metric elbo_validation did not improve in the last 45 records. Best score: 34.337. Signaling Trainer to stop.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "epoch 4/10000 (batch 1000) - train loss: 7.1475, val loss: 7.1613\n",
      "epoch 8/10000 (batch 2000) - train loss: 7.0417, val loss: 7.0411\n",
      "epoch 11/10000 (batch 3000) - train loss: 6.8623, val loss: 6.8450\n",
      "epoch 15/10000 (batch 4000) - train loss: 6.5729, val loss: 6.5713\n",
      "epoch 18/10000 (batch 5000) - train loss: 6.2508, val loss: 6.2205\n",
      "epoch 22/10000 (batch 6000) - train loss: 5.7960, val loss: 5.7917\n",
      "epoch 25/10000 (batch 7000) - train loss: 5.3290, val loss: 5.2854\n",
      "epoch 29/10000 (batch 8000) - train loss: 4.7084, val loss: 4.7013\n",
      "epoch 32/10000 (batch 9000) - train loss: 4.0989, val loss: 4.0400\n",
      "epoch 36/10000 (batch 10000) - train loss: 3.3236, val loss: 3.3133\n",
      "epoch 39/10000 (batch 11000) - train loss: 2.6028, val loss: 2.5354\n",
      "epoch 43/10000 (batch 12000) - train loss: 1.7774, val loss: 1.7657\n",
      "epoch 46/10000 (batch 13000) - train loss: 1.1574, val loss: 1.1066\n",
      "epoch 50/10000 (batch 14000) - train loss: 0.6552, val loss: 0.6447\n",
      "epoch 53/10000 (batch 15000) - train loss: 0.4027, val loss: 0.3815\n",
      "epoch 57/10000 (batch 16000) - train loss: 0.2502, val loss: 0.2496\n",
      "epoch 60/10000 (batch 17000) - train loss: 0.1886, val loss: 0.1794\n",
      "epoch 64/10000 (batch 18000) - train loss: 0.1495, val loss: 0.1471\n",
      "epoch 67/10000 (batch 19000) - train loss: 0.1281, val loss: 0.1265\n",
      "epoch 71/10000 (batch 20000) - train loss: 0.1202, val loss: 0.1152\n",
      "epoch 74/10000 (batch 21000) - train loss: 0.1126, val loss: 0.1077\n",
      "epoch 78/10000 (batch 22000) - train loss: 0.1076, val loss: 0.1039\n",
      "epoch 81/10000 (batch 23000) - train loss: 0.1061, val loss: 0.1041\n",
      "Early stopping triggered at epoch 81, batch 23000\n",
      "/home/gokul/.local/lib/python3.10/site-packages/scvi/model/_utils.py:286: UserWarning: This dataset has some empty cells, this might fail inference.Data should be filtered with `scanpy.pp.filter_cells()`\n",
      "  warnings.warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Monitored metric elbo_validation did not improve in the last 45 records. Best score: 34.237. Signaling Trainer to stop.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "/home/gokul/.local/lib/python3.10/site-packages/scvi/model/_utils.py:286: UserWarning: This dataset has some empty cells, this might fail inference.Data should be filtered with `scanpy.pp.filter_cells()`\n",
      "  warnings.warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Monitored metric elbo_validation did not improve in the last 45 records. Best score: 35.906. Signaling Trainer to stop.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "/home/gokul/.local/lib/python3.10/site-packages/scvi/model/_utils.py:286: UserWarning: This dataset has some empty cells, this might fail inference.Data should be filtered with `scanpy.pp.filter_cells()`\n",
      "  warnings.warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Monitored metric elbo_validation did not improve in the last 45 records. Best score: 36.601. Signaling Trainer to stop.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "epoch 1/10000 (batch 1000) - train loss: 6.8760, val loss: 6.8815\n",
      "epoch 1/10000 (batch 2000) - train loss: 6.8761, val loss: 6.8695\n",
      "epoch 2/10000 (batch 3000) - train loss: 6.8514, val loss: 6.8493\n",
      "epoch 2/10000 (batch 4000) - train loss: 6.8384, val loss: 6.8219\n",
      "epoch 2/10000 (batch 5000) - train loss: 6.8228, val loss: 6.7864\n",
      "epoch 3/10000 (batch 6000) - train loss: 6.7501, val loss: 6.7433\n",
      "epoch 3/10000 (batch 7000) - train loss: 6.7255, val loss: 6.6919\n",
      "epoch 3/10000 (batch 8000) - train loss: 6.6984, val loss: 6.6329\n",
      "epoch 4/10000 (batch 9000) - train loss: 6.5817, val loss: 6.5662\n",
      "epoch 4/10000 (batch 10000) - train loss: 6.5455, val loss: 6.4912\n",
      "epoch 4/10000 (batch 11000) - train loss: 6.5067, val loss: 6.4086\n",
      "epoch 5/10000 (batch 12000) - train loss: 6.3459, val loss: 6.3185\n",
      "epoch 5/10000 (batch 13000) - train loss: 6.2981, val loss: 6.2197\n",
      "epoch 5/10000 (batch 14000) - train loss: 6.2477, val loss: 6.1136\n",
      "epoch 6/10000 (batch 15000) - train loss: 6.0429, val loss: 5.9995\n",
      "epoch 6/10000 (batch 16000) - train loss: 5.9834, val loss: 5.8775\n",
      "epoch 6/10000 (batch 17000) - train loss: 5.9214, val loss: 5.7476\n",
      "epoch 7/10000 (batch 18000) - train loss: 5.6725, val loss: 5.6100\n",
      "epoch 7/10000 (batch 19000) - train loss: 5.6015, val loss: 5.4646\n",
      "epoch 8/10000 (batch 20000) - train loss: 5.3145, val loss: 5.3112\n",
      "epoch 8/10000 (batch 21000) - train loss: 5.2347, val loss: 5.1502\n",
      "epoch 8/10000 (batch 22000) - train loss: 5.1525, val loss: 4.9813\n",
      "epoch 9/10000 (batch 23000) - train loss: 4.8222, val loss: 4.8048\n",
      "epoch 9/10000 (batch 24000) - train loss: 4.7307, val loss: 4.6205\n",
      "epoch 9/10000 (batch 25000) - train loss: 4.6369, val loss: 4.4286\n",
      "epoch 10/10000 (batch 26000) - train loss: 4.2633, val loss: 4.2286\n",
      "epoch 10/10000 (batch 27000) - train loss: 4.1611, val loss: 4.0218\n",
      "epoch 10/10000 (batch 28000) - train loss: 4.0562, val loss: 3.8083\n",
      "epoch 11/10000 (batch 29000) - train loss: 3.6419, val loss: 3.5874\n",
      "epoch 11/10000 (batch 30000) - train loss: 3.5294, val loss: 3.3596\n",
      "epoch 11/10000 (batch 31000) - train loss: 3.4148, val loss: 3.1274\n",
      "epoch 12/10000 (batch 32000) - train loss: 2.9659, val loss: 2.8894\n",
      "epoch 12/10000 (batch 33000) - train loss: 2.8456, val loss: 2.6479\n",
      "epoch 12/10000 (batch 34000) - train loss: 2.7245, val loss: 2.4038\n",
      "epoch 13/10000 (batch 35000) - train loss: 2.2558, val loss: 2.1598\n",
      "epoch 13/10000 (batch 36000) - train loss: 2.1345, val loss: 1.9185\n",
      "epoch 13/10000 (batch 37000) - train loss: 2.0146, val loss: 1.6822\n",
      "epoch 14/10000 (batch 38000) - train loss: 1.5612, val loss: 1.4556\n",
      "epoch 14/10000 (batch 39000) - train loss: 1.4509, val loss: 1.2419\n",
      "epoch 15/10000 (batch 40000) - train loss: 1.0510, val loss: 1.0448\n",
      "epoch 15/10000 (batch 41000) - train loss: 0.9630, val loss: 0.8691\n",
      "epoch 15/10000 (batch 42000) - train loss: 0.8798, val loss: 0.7152\n",
      "epoch 16/10000 (batch 43000) - train loss: 0.5975, val loss: 0.5851\n",
      "epoch 16/10000 (batch 44000) - train loss: 0.5427, val loss: 0.4770\n",
      "epoch 16/10000 (batch 45000) - train loss: 0.4930, val loss: 0.3892\n",
      "epoch 17/10000 (batch 46000) - train loss: 0.3335, val loss: 0.3206\n",
      "epoch 17/10000 (batch 47000) - train loss: 0.3037, val loss: 0.2670\n",
      "epoch 17/10000 (batch 48000) - train loss: 0.2792, val loss: 0.2254\n",
      "epoch 18/10000 (batch 49000) - train loss: 0.2016, val loss: 0.1930\n",
      "epoch 18/10000 (batch 50000) - train loss: 0.1883, val loss: 0.1699\n",
      "epoch 18/10000 (batch 51000) - train loss: 0.1775, val loss: 0.1515\n",
      "epoch 19/10000 (batch 52000) - train loss: 0.1444, val loss: 0.1393\n",
      "epoch 19/10000 (batch 53000) - train loss: 0.1385, val loss: 0.1305\n",
      "epoch 19/10000 (batch 54000) - train loss: 0.1340, val loss: 0.1233\n",
      "epoch 20/10000 (batch 55000) - train loss: 0.1197, val loss: 0.1183\n",
      "epoch 20/10000 (batch 56000) - train loss: 0.1182, val loss: 0.1145\n",
      "epoch 20/10000 (batch 57000) - train loss: 0.1165, val loss: 0.1129\n",
      "epoch 21/10000 (batch 58000) - train loss: 0.1121, val loss: 0.1114\n",
      "epoch 21/10000 (batch 59000) - train loss: 0.1104, val loss: 0.1093\n",
      "epoch 22/10000 (batch 60000) - train loss: 0.1074, val loss: 0.1080\n",
      "epoch 22/10000 (batch 61000) - train loss: 0.1083, val loss: 0.1070\n",
      "epoch 22/10000 (batch 62000) - train loss: 0.1074, val loss: 0.1064\n",
      "epoch 23/10000 (batch 63000) - train loss: 0.1073, val loss: 0.1050\n",
      "epoch 23/10000 (batch 64000) - train loss: 0.1059, val loss: 0.1049\n",
      "epoch 23/10000 (batch 65000) - train loss: 0.1052, val loss: 0.1046\n",
      "epoch 24/10000 (batch 66000) - train loss: 0.1046, val loss: 0.1038\n",
      "epoch 24/10000 (batch 67000) - train loss: 0.1039, val loss: 0.1031\n",
      "epoch 24/10000 (batch 68000) - train loss: 0.1034, val loss: 0.1012\n",
      "epoch 25/10000 (batch 69000) - train loss: 0.1019, val loss: 0.1018\n",
      "Early stopping triggered at epoch 25, batch 69000\n",
      "embedding generation completed! :) \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "358060be9de545b1abafaf56e050b9f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n",
      "/home/gokul/.local/lib/python3.10/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n",
      "fractions:   0%|          | 0/10 [00:00<?, ?it/s]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/home/gokul/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "Monitored metric elbo_validation did not improve in the last 45 records. Best score: 44.653. Signaling Trainer to stop.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "epoch 500/10000 (batch 1000) - train loss: 2.0261, val loss: 1.8226\n",
      "epoch 1000/10000 (batch 2000) - train loss: 0.1746, val loss: 0.2256\n",
      "epoch 1500/10000 (batch 3000) - train loss: 0.1535, val loss: 0.2232\n",
      "epoch 2000/10000 (batch 4000) - train loss: 0.1169, val loss: 0.1607\n",
      "epoch 2500/10000 (batch 5000) - train loss: 0.1194, val loss: 0.1691\n",
      "Early stopping triggered at epoch 2500, batch 5000\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/home/gokul/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "Monitored metric elbo_validation did not improve in the last 45 records. Best score: 76.916. Signaling Trainer to stop.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/home/gokul/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "Monitored metric elbo_validation did not improve in the last 45 records. Best score: 56.900. Signaling Trainer to stop.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/home/gokul/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "Monitored metric elbo_validation did not improve in the last 45 records. Best score: 51.687. Signaling Trainer to stop.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "epoch 36/10000 (batch 1000) - train loss: 6.5761, val loss: 6.5561\n",
      "epoch 72/10000 (batch 2000) - train loss: 5.3305, val loss: 5.3234\n",
      "epoch 108/10000 (batch 3000) - train loss: 3.3034, val loss: 3.3106\n",
      "epoch 143/10000 (batch 4000) - train loss: 1.0952, val loss: 1.0919\n",
      "epoch 179/10000 (batch 5000) - train loss: 0.3487, val loss: 0.3560\n",
      "epoch 215/10000 (batch 6000) - train loss: 0.2322, val loss: 0.2390\n",
      "epoch 250/10000 (batch 7000) - train loss: 0.1853, val loss: 0.2128\n",
      "epoch 286/10000 (batch 8000) - train loss: 0.1702, val loss: 0.1970\n",
      "epoch 322/10000 (batch 9000) - train loss: 0.1482, val loss: 0.1800\n",
      "epoch 358/10000 (batch 10000) - train loss: 0.1536, val loss: 0.1587\n",
      "epoch 393/10000 (batch 11000) - train loss: 0.1359, val loss: 0.1481\n",
      "epoch 429/10000 (batch 12000) - train loss: 0.1314, val loss: 0.1326\n",
      "epoch 465/10000 (batch 13000) - train loss: 0.1142, val loss: 0.1255\n",
      "epoch 500/10000 (batch 14000) - train loss: 0.1142, val loss: 0.1187\n",
      "epoch 536/10000 (batch 15000) - train loss: 0.1202, val loss: 0.1281\n",
      "Early stopping triggered at epoch 536, batch 15000\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/home/gokul/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "Monitored metric elbo_validation did not improve in the last 45 records. Best score: 53.875. Signaling Trainer to stop.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Monitored metric elbo_validation did not improve in the last 45 records. Best score: 55.882. Signaling Trainer to stop.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "`Trainer.fit` stopped: `max_epochs=400` reached.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "epoch 4/10000 (batch 1000) - train loss: 7.3731, val loss: 7.3970\n",
      "epoch 8/10000 (batch 2000) - train loss: 7.2805, val loss: 7.2795\n",
      "epoch 11/10000 (batch 3000) - train loss: 7.1035, val loss: 7.0866\n",
      "epoch 15/10000 (batch 4000) - train loss: 6.8202, val loss: 6.8184\n",
      "epoch 18/10000 (batch 5000) - train loss: 6.5040, val loss: 6.4745\n",
      "epoch 22/10000 (batch 6000) - train loss: 6.0583, val loss: 6.0542\n",
      "epoch 25/10000 (batch 7000) - train loss: 5.5998, val loss: 5.5566\n",
      "epoch 29/10000 (batch 8000) - train loss: 4.9886, val loss: 4.9819\n",
      "epoch 32/10000 (batch 9000) - train loss: 4.3874, val loss: 4.3289\n",
      "epoch 36/10000 (batch 10000) - train loss: 3.6168, val loss: 3.6055\n",
      "epoch 39/10000 (batch 11000) - train loss: 2.8947, val loss: 2.8269\n",
      "epoch 43/10000 (batch 12000) - train loss: 2.0437, val loss: 2.0332\n",
      "epoch 46/10000 (batch 13000) - train loss: 1.3790, val loss: 1.3200\n",
      "epoch 50/10000 (batch 14000) - train loss: 0.8106, val loss: 0.7977\n",
      "epoch 53/10000 (batch 15000) - train loss: 0.5167, val loss: 0.4938\n",
      "epoch 57/10000 (batch 16000) - train loss: 0.3479, val loss: 0.3405\n",
      "epoch 60/10000 (batch 17000) - train loss: 0.2699, val loss: 0.2597\n",
      "epoch 64/10000 (batch 18000) - train loss: 0.2243, val loss: 0.2185\n",
      "epoch 67/10000 (batch 19000) - train loss: 0.2069, val loss: 0.1967\n",
      "epoch 71/10000 (batch 20000) - train loss: 0.1886, val loss: 0.1846\n",
      "epoch 74/10000 (batch 21000) - train loss: 0.1843, val loss: 0.1790\n",
      "epoch 78/10000 (batch 22000) - train loss: 0.1785, val loss: 0.1764\n",
      "epoch 81/10000 (batch 23000) - train loss: 0.1767, val loss: 0.1736\n",
      "epoch 85/10000 (batch 24000) - train loss: 0.1689, val loss: 0.1670\n",
      "epoch 88/10000 (batch 25000) - train loss: 0.1728, val loss: 0.1660\n",
      "epoch 92/10000 (batch 26000) - train loss: 0.1660, val loss: 0.1606\n",
      "epoch 95/10000 (batch 27000) - train loss: 0.1555, val loss: 0.1541\n",
      "epoch 99/10000 (batch 28000) - train loss: 0.1461, val loss: 0.1465\n",
      "epoch 102/10000 (batch 29000) - train loss: 0.1456, val loss: 0.1441\n",
      "epoch 106/10000 (batch 30000) - train loss: 0.1377, val loss: 0.1353\n",
      "epoch 109/10000 (batch 31000) - train loss: 0.1328, val loss: 0.1303\n",
      "epoch 113/10000 (batch 32000) - train loss: 0.1249, val loss: 0.1244\n",
      "epoch 116/10000 (batch 33000) - train loss: 0.1244, val loss: 0.1224\n",
      "epoch 120/10000 (batch 34000) - train loss: 0.1222, val loss: 0.1199\n",
      "epoch 123/10000 (batch 35000) - train loss: 0.1208, val loss: 0.1171\n",
      "epoch 127/10000 (batch 36000) - train loss: 0.1177, val loss: 0.1148\n",
      "epoch 130/10000 (batch 37000) - train loss: 0.1190, val loss: 0.1188\n",
      "Early stopping triggered at epoch 130, batch 37000\n",
      "/home/gokul/.local/lib/python3.10/site-packages/scvi/model/_utils.py:286: UserWarning: This dataset has some empty cells, this might fail inference.Data should be filtered with `scanpy.pp.filter_cells()`\n",
      "  warnings.warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "`Trainer.fit` stopped: `max_epochs=326` reached.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "/home/gokul/.local/lib/python3.10/site-packages/scvi/model/_utils.py:286: UserWarning: This dataset has some empty cells, this might fail inference.Data should be filtered with `scanpy.pp.filter_cells()`\n",
      "  warnings.warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "`Trainer.fit` stopped: `max_epochs=151` reached.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "/home/gokul/.local/lib/python3.10/site-packages/scvi/model/_utils.py:286: UserWarning: This dataset has some empty cells, this might fail inference.Data should be filtered with `scanpy.pp.filter_cells()`\n",
      "  warnings.warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "`Trainer.fit` stopped: `max_epochs=70` reached.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "epoch 1/10000 (batch 1000) - train loss: 7.4448, val loss: 7.4762\n",
      "epoch 1/10000 (batch 2000) - train loss: 7.4580, val loss: 7.4642\n",
      "epoch 2/10000 (batch 3000) - train loss: 7.4468, val loss: 7.4451\n",
      "epoch 2/10000 (batch 4000) - train loss: 7.4342, val loss: 7.4181\n",
      "epoch 2/10000 (batch 5000) - train loss: 7.4190, val loss: 7.3834\n",
      "epoch 3/10000 (batch 6000) - train loss: 7.3482, val loss: 7.3415\n",
      "epoch 3/10000 (batch 7000) - train loss: 7.3242, val loss: 7.2916\n",
      "epoch 3/10000 (batch 8000) - train loss: 7.2977, val loss: 7.2336\n",
      "epoch 4/10000 (batch 9000) - train loss: 7.1839, val loss: 7.1687\n",
      "epoch 4/10000 (batch 10000) - train loss: 7.1487, val loss: 7.0957\n",
      "epoch 4/10000 (batch 11000) - train loss: 7.1109, val loss: 7.0154\n",
      "epoch 5/10000 (batch 12000) - train loss: 6.9541, val loss: 6.9268\n",
      "epoch 5/10000 (batch 13000) - train loss: 6.9075, val loss: 6.8310\n",
      "epoch 5/10000 (batch 14000) - train loss: 6.8584, val loss: 6.7275\n",
      "epoch 6/10000 (batch 15000) - train loss: 6.6585, val loss: 6.6165\n",
      "epoch 6/10000 (batch 16000) - train loss: 6.6006, val loss: 6.4974\n",
      "epoch 6/10000 (batch 17000) - train loss: 6.5402, val loss: 6.3708\n",
      "epoch 7/10000 (batch 18000) - train loss: 6.2971, val loss: 6.2368\n",
      "epoch 7/10000 (batch 19000) - train loss: 6.2280, val loss: 6.0943\n",
      "epoch 8/10000 (batch 20000) - train loss: 5.9487, val loss: 5.9450\n",
      "epoch 8/10000 (batch 21000) - train loss: 5.8699, val loss: 5.7873\n",
      "epoch 8/10000 (batch 22000) - train loss: 5.7894, val loss: 5.6226\n",
      "epoch 9/10000 (batch 23000) - train loss: 5.4667, val loss: 5.4500\n",
      "epoch 9/10000 (batch 24000) - train loss: 5.3770, val loss: 5.2690\n",
      "epoch 9/10000 (batch 25000) - train loss: 5.2852, val loss: 5.0812\n",
      "epoch 10/10000 (batch 26000) - train loss: 4.9188, val loss: 4.8853\n",
      "epoch 10/10000 (batch 27000) - train loss: 4.8185, val loss: 4.6822\n",
      "epoch 10/10000 (batch 28000) - train loss: 4.7155, val loss: 4.4713\n",
      "epoch 11/10000 (batch 29000) - train loss: 4.3067, val loss: 4.2531\n",
      "epoch 11/10000 (batch 30000) - train loss: 4.1952, val loss: 4.0274\n",
      "epoch 11/10000 (batch 31000) - train loss: 4.0816, val loss: 3.7957\n",
      "epoch 12/10000 (batch 32000) - train loss: 3.6340, val loss: 3.5569\n",
      "epoch 12/10000 (batch 33000) - train loss: 3.5119, val loss: 3.3120\n",
      "epoch 12/10000 (batch 34000) - train loss: 3.3890, val loss: 3.0623\n",
      "epoch 13/10000 (batch 35000) - train loss: 2.9081, val loss: 2.8080\n",
      "epoch 13/10000 (batch 36000) - train loss: 2.7808, val loss: 2.5524\n",
      "epoch 13/10000 (batch 37000) - train loss: 2.6525, val loss: 2.2943\n",
      "epoch 14/10000 (batch 38000) - train loss: 2.1589, val loss: 2.0408\n",
      "epoch 14/10000 (batch 39000) - train loss: 2.0327, val loss: 1.7909\n",
      "epoch 15/10000 (batch 40000) - train loss: 1.5584, val loss: 1.5524\n",
      "epoch 15/10000 (batch 41000) - train loss: 1.4477, val loss: 1.3273\n",
      "epoch 15/10000 (batch 42000) - train loss: 1.3397, val loss: 1.1217\n",
      "epoch 16/10000 (batch 43000) - train loss: 0.9532, val loss: 0.9377\n",
      "epoch 16/10000 (batch 44000) - train loss: 0.8736, val loss: 0.7778\n",
      "epoch 16/10000 (batch 45000) - train loss: 0.7998, val loss: 0.6435\n",
      "epoch 17/10000 (batch 46000) - train loss: 0.5510, val loss: 0.5340\n",
      "epoch 17/10000 (batch 47000) - train loss: 0.5050, val loss: 0.4457\n",
      "epoch 17/10000 (batch 48000) - train loss: 0.4649, val loss: 0.3788\n",
      "epoch 18/10000 (batch 49000) - train loss: 0.3382, val loss: 0.3246\n",
      "epoch 18/10000 (batch 50000) - train loss: 0.3161, val loss: 0.2882\n",
      "epoch 18/10000 (batch 51000) - train loss: 0.2979, val loss: 0.2561\n",
      "epoch 19/10000 (batch 52000) - train loss: 0.2430, val loss: 0.2350\n",
      "epoch 19/10000 (batch 53000) - train loss: 0.2321, val loss: 0.2196\n",
      "epoch 19/10000 (batch 54000) - train loss: 0.2245, val loss: 0.2067\n",
      "epoch 20/10000 (batch 55000) - train loss: 0.2029, val loss: 0.1999\n",
      "epoch 20/10000 (batch 56000) - train loss: 0.1991, val loss: 0.1940\n",
      "epoch 20/10000 (batch 57000) - train loss: 0.1958, val loss: 0.1893\n",
      "epoch 21/10000 (batch 58000) - train loss: 0.1869, val loss: 0.1877\n",
      "epoch 21/10000 (batch 59000) - train loss: 0.1861, val loss: 0.1856\n",
      "epoch 22/10000 (batch 60000) - train loss: 0.1757, val loss: 0.1837\n",
      "epoch 22/10000 (batch 61000) - train loss: 0.1788, val loss: 0.1809\n",
      "epoch 22/10000 (batch 62000) - train loss: 0.1791, val loss: 0.1803\n",
      "epoch 23/10000 (batch 63000) - train loss: 0.1787, val loss: 0.1779\n",
      "epoch 23/10000 (batch 64000) - train loss: 0.1769, val loss: 0.1763\n",
      "epoch 23/10000 (batch 65000) - train loss: 0.1770, val loss: 0.1769\n",
      "Early stopping triggered at epoch 23, batch 65000\n",
      "embedding generation completed! :) \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1368d02b9c0a4a439ee9c1115a4fae47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n",
      "/home/gokul/.local/lib/python3.10/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n",
      "fractions:   0%|          | 0/10 [00:00<?, ?it/s]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/home/gokul/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "Monitored metric elbo_validation did not improve in the last 45 records. Best score: 83.222. Signaling Trainer to stop.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "epoch 500/10000 (batch 1000) - train loss: 1.9478, val loss: 1.6933\n",
      "epoch 1000/10000 (batch 2000) - train loss: 0.2753, val loss: 0.2475\n",
      "epoch 1500/10000 (batch 3000) - train loss: 0.2270, val loss: 0.1505\n",
      "epoch 2000/10000 (batch 4000) - train loss: 0.2102, val loss: 0.1813\n",
      "Early stopping triggered at epoch 2000, batch 4000\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/home/gokul/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "Monitored metric elbo_validation did not improve in the last 45 records. Best score: 109.941. Signaling Trainer to stop.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/home/gokul/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=400` reached.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/home/gokul/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "Monitored metric elbo_validation did not improve in the last 45 records. Best score: 80.638. Signaling Trainer to stop.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "epoch 36/10000 (batch 1000) - train loss: 6.4171, val loss: 6.3912\n",
      "epoch 72/10000 (batch 2000) - train loss: 5.1520, val loss: 5.1427\n",
      "epoch 108/10000 (batch 3000) - train loss: 3.1142, val loss: 3.1129\n",
      "epoch 143/10000 (batch 4000) - train loss: 1.0454, val loss: 1.0421\n",
      "epoch 179/10000 (batch 5000) - train loss: 0.4336, val loss: 0.4208\n",
      "epoch 215/10000 (batch 6000) - train loss: 0.3071, val loss: 0.3250\n",
      "epoch 250/10000 (batch 7000) - train loss: 0.2781, val loss: 0.2939\n",
      "epoch 286/10000 (batch 8000) - train loss: 0.2634, val loss: 0.2684\n",
      "epoch 322/10000 (batch 9000) - train loss: 0.2333, val loss: 0.2543\n",
      "epoch 358/10000 (batch 10000) - train loss: 0.2182, val loss: 0.2466\n",
      "epoch 393/10000 (batch 11000) - train loss: 0.2075, val loss: 0.2257\n",
      "epoch 429/10000 (batch 12000) - train loss: 0.2039, val loss: 0.1933\n",
      "epoch 465/10000 (batch 13000) - train loss: 0.1851, val loss: 0.2052\n",
      "Early stopping triggered at epoch 465, batch 13000\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/home/gokul/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "Monitored metric elbo_validation did not improve in the last 45 records. Best score: 75.521. Signaling Trainer to stop.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "`Trainer.fit` stopped: `max_epochs=400` reached.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "`Trainer.fit` stopped: `max_epochs=400` reached.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "epoch 4/10000 (batch 1000) - train loss: 6.5246, val loss: 6.5058\n",
      "epoch 8/10000 (batch 2000) - train loss: 6.3898, val loss: 6.3894\n",
      "epoch 11/10000 (batch 3000) - train loss: 6.2162, val loss: 6.1993\n",
      "epoch 15/10000 (batch 4000) - train loss: 5.9355, val loss: 5.9339\n",
      "epoch 18/10000 (batch 5000) - train loss: 5.6228, val loss: 5.5931\n",
      "epoch 22/10000 (batch 6000) - train loss: 5.1805, val loss: 5.1759\n",
      "epoch 25/10000 (batch 7000) - train loss: 4.7266, val loss: 4.6825\n",
      "epoch 29/10000 (batch 8000) - train loss: 4.1234, val loss: 4.1145\n",
      "epoch 32/10000 (batch 9000) - train loss: 3.5321, val loss: 3.4771\n",
      "epoch 36/10000 (batch 10000) - train loss: 2.7952, val loss: 2.7819\n",
      "epoch 39/10000 (batch 11000) - train loss: 2.1275, val loss: 2.0685\n",
      "epoch 43/10000 (batch 12000) - train loss: 1.4209, val loss: 1.4156\n",
      "epoch 46/10000 (batch 13000) - train loss: 0.9545, val loss: 0.9188\n",
      "epoch 50/10000 (batch 14000) - train loss: 0.6191, val loss: 0.6164\n",
      "epoch 53/10000 (batch 15000) - train loss: 0.4705, val loss: 0.4554\n",
      "epoch 57/10000 (batch 16000) - train loss: 0.3761, val loss: 0.3732\n",
      "epoch 60/10000 (batch 17000) - train loss: 0.3364, val loss: 0.3362\n",
      "epoch 64/10000 (batch 18000) - train loss: 0.3217, val loss: 0.3094\n",
      "epoch 67/10000 (batch 19000) - train loss: 0.3019, val loss: 0.3016\n",
      "epoch 71/10000 (batch 20000) - train loss: 0.2920, val loss: 0.2918\n",
      "epoch 74/10000 (batch 21000) - train loss: 0.2877, val loss: 0.2837\n",
      "epoch 78/10000 (batch 22000) - train loss: 0.2864, val loss: 0.2803\n",
      "epoch 81/10000 (batch 23000) - train loss: 0.2751, val loss: 0.2748\n",
      "epoch 85/10000 (batch 24000) - train loss: 0.2583, val loss: 0.2679\n",
      "epoch 88/10000 (batch 25000) - train loss: 0.2624, val loss: 0.2568\n",
      "epoch 92/10000 (batch 26000) - train loss: 0.2513, val loss: 0.2511\n",
      "epoch 95/10000 (batch 27000) - train loss: 0.2415, val loss: 0.2427\n",
      "epoch 99/10000 (batch 28000) - train loss: 0.2331, val loss: 0.2291\n",
      "epoch 102/10000 (batch 29000) - train loss: 0.2218, val loss: 0.2263\n",
      "epoch 106/10000 (batch 30000) - train loss: 0.2228, val loss: 0.2182\n",
      "epoch 109/10000 (batch 31000) - train loss: 0.2117, val loss: 0.2112\n",
      "epoch 113/10000 (batch 32000) - train loss: 0.2101, val loss: 0.2084\n",
      "epoch 116/10000 (batch 33000) - train loss: 0.2050, val loss: 0.2043\n",
      "epoch 120/10000 (batch 34000) - train loss: 0.1973, val loss: 0.2053\n",
      "Early stopping triggered at epoch 120, batch 34000\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "`Trainer.fit` stopped: `max_epochs=326` reached.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "`Trainer.fit` stopped: `max_epochs=151` reached.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "`Trainer.fit` stopped: `max_epochs=70` reached.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "epoch 1/10000 (batch 1000) - train loss: 6.7925, val loss: 6.7883\n",
      "epoch 1/10000 (batch 2000) - train loss: 6.7878, val loss: 6.7771\n",
      "epoch 2/10000 (batch 3000) - train loss: 6.7595, val loss: 6.7576\n",
      "epoch 2/10000 (batch 4000) - train loss: 6.7472, val loss: 6.7315\n",
      "epoch 2/10000 (batch 5000) - train loss: 6.7323, val loss: 6.6976\n",
      "epoch 3/10000 (batch 6000) - train loss: 6.6629, val loss: 6.6559\n",
      "epoch 3/10000 (batch 7000) - train loss: 6.6395, val loss: 6.6077\n",
      "epoch 3/10000 (batch 8000) - train loss: 6.6137, val loss: 6.5510\n",
      "epoch 4/10000 (batch 9000) - train loss: 6.5023, val loss: 6.4875\n",
      "epoch 4/10000 (batch 10000) - train loss: 6.4679, val loss: 6.4162\n",
      "epoch 4/10000 (batch 11000) - train loss: 6.4310, val loss: 6.3374\n",
      "epoch 5/10000 (batch 12000) - train loss: 6.2774, val loss: 6.2511\n",
      "epoch 5/10000 (batch 13000) - train loss: 6.2320, val loss: 6.1574\n",
      "epoch 5/10000 (batch 14000) - train loss: 6.1841, val loss: 6.0560\n",
      "epoch 6/10000 (batch 15000) - train loss: 5.9888, val loss: 5.9477\n",
      "epoch 6/10000 (batch 16000) - train loss: 5.9324, val loss: 5.8316\n",
      "epoch 6/10000 (batch 17000) - train loss: 5.8734, val loss: 5.7075\n",
      "epoch 7/10000 (batch 18000) - train loss: 5.6359, val loss: 5.5767\n",
      "epoch 7/10000 (batch 19000) - train loss: 5.5687, val loss: 5.4387\n",
      "epoch 8/10000 (batch 20000) - train loss: 5.2959, val loss: 5.2922\n",
      "epoch 8/10000 (batch 21000) - train loss: 5.2194, val loss: 5.1388\n",
      "epoch 8/10000 (batch 22000) - train loss: 5.1413, val loss: 4.9782\n",
      "epoch 9/10000 (batch 23000) - train loss: 4.8265, val loss: 4.8097\n",
      "epoch 9/10000 (batch 24000) - train loss: 4.7398, val loss: 4.6345\n",
      "epoch 9/10000 (batch 25000) - train loss: 4.6505, val loss: 4.4521\n",
      "epoch 10/10000 (batch 26000) - train loss: 4.2950, val loss: 4.2614\n",
      "epoch 10/10000 (batch 27000) - train loss: 4.1970, val loss: 4.0639\n",
      "epoch 10/10000 (batch 28000) - train loss: 4.0969, val loss: 3.8607\n",
      "epoch 11/10000 (batch 29000) - train loss: 3.7033, val loss: 3.6503\n",
      "epoch 11/10000 (batch 30000) - train loss: 3.5955, val loss: 3.4324\n",
      "epoch 11/10000 (batch 31000) - train loss: 3.4860, val loss: 3.2099\n",
      "epoch 12/10000 (batch 32000) - train loss: 3.0570, val loss: 2.9837\n",
      "epoch 12/10000 (batch 33000) - train loss: 2.9418, val loss: 2.7517\n",
      "epoch 12/10000 (batch 34000) - train loss: 2.8256, val loss: 2.5188\n",
      "epoch 13/10000 (batch 35000) - train loss: 2.3781, val loss: 2.2853\n",
      "epoch 13/10000 (batch 36000) - train loss: 2.2605, val loss: 2.0526\n",
      "epoch 13/10000 (batch 37000) - train loss: 2.1447, val loss: 1.8241\n",
      "epoch 14/10000 (batch 38000) - train loss: 1.7087, val loss: 1.6064\n",
      "epoch 14/10000 (batch 39000) - train loss: 1.6017, val loss: 1.3994\n",
      "epoch 15/10000 (batch 40000) - train loss: 1.2137, val loss: 1.2081\n",
      "epoch 15/10000 (batch 41000) - train loss: 1.1291, val loss: 1.0356\n",
      "epoch 15/10000 (batch 42000) - train loss: 1.0487, val loss: 0.8860\n",
      "epoch 16/10000 (batch 43000) - train loss: 0.7727, val loss: 0.7578\n",
      "epoch 16/10000 (batch 44000) - train loss: 0.7176, val loss: 0.6504\n",
      "epoch 16/10000 (batch 45000) - train loss: 0.6680, val loss: 0.5660\n",
      "epoch 17/10000 (batch 46000) - train loss: 0.5113, val loss: 0.4968\n",
      "epoch 17/10000 (batch 47000) - train loss: 0.4817, val loss: 0.4455\n",
      "epoch 17/10000 (batch 48000) - train loss: 0.4574, val loss: 0.4038\n",
      "epoch 18/10000 (batch 49000) - train loss: 0.3842, val loss: 0.3748\n",
      "epoch 18/10000 (batch 50000) - train loss: 0.3699, val loss: 0.3522\n",
      "epoch 18/10000 (batch 51000) - train loss: 0.3598, val loss: 0.3347\n",
      "epoch 19/10000 (batch 52000) - train loss: 0.3284, val loss: 0.3239\n",
      "epoch 19/10000 (batch 53000) - train loss: 0.3230, val loss: 0.3152\n",
      "epoch 19/10000 (batch 54000) - train loss: 0.3182, val loss: 0.3091\n",
      "epoch 20/10000 (batch 55000) - train loss: 0.3067, val loss: 0.3040\n",
      "epoch 20/10000 (batch 56000) - train loss: 0.3041, val loss: 0.3003\n",
      "epoch 20/10000 (batch 57000) - train loss: 0.3025, val loss: 0.2981\n",
      "epoch 21/10000 (batch 58000) - train loss: 0.2966, val loss: 0.2943\n",
      "epoch 21/10000 (batch 59000) - train loss: 0.2955, val loss: 0.2913\n",
      "epoch 22/10000 (batch 60000) - train loss: 0.2956, val loss: 0.2886\n",
      "epoch 22/10000 (batch 61000) - train loss: 0.2905, val loss: 0.2879\n",
      "epoch 22/10000 (batch 62000) - train loss: 0.2889, val loss: 0.2873\n",
      "epoch 23/10000 (batch 63000) - train loss: 0.2898, val loss: 0.2843\n",
      "epoch 23/10000 (batch 64000) - train loss: 0.2860, val loss: 0.2836\n",
      "epoch 23/10000 (batch 65000) - train loss: 0.2833, val loss: 0.2812\n",
      "epoch 24/10000 (batch 66000) - train loss: 0.2797, val loss: 0.2785\n",
      "epoch 24/10000 (batch 67000) - train loss: 0.2785, val loss: 0.2784\n",
      "epoch 24/10000 (batch 68000) - train loss: 0.2784, val loss: 0.2751\n",
      "epoch 25/10000 (batch 69000) - train loss: 0.2761, val loss: 0.2734\n",
      "epoch 25/10000 (batch 70000) - train loss: 0.2720, val loss: 0.2717\n",
      "epoch 25/10000 (batch 71000) - train loss: 0.2718, val loss: 0.2685\n",
      "epoch 26/10000 (batch 72000) - train loss: 0.2675, val loss: 0.2674\n",
      "epoch 26/10000 (batch 73000) - train loss: 0.2665, val loss: 0.2639\n",
      "epoch 26/10000 (batch 74000) - train loss: 0.2662, val loss: 0.2628\n",
      "epoch 27/10000 (batch 75000) - train loss: 0.2630, val loss: 0.2599\n",
      "epoch 27/10000 (batch 76000) - train loss: 0.2604, val loss: 0.2588\n",
      "epoch 28/10000 (batch 77000) - train loss: 0.2588, val loss: 0.2552\n",
      "epoch 28/10000 (batch 78000) - train loss: 0.2541, val loss: 0.2529\n",
      "epoch 28/10000 (batch 79000) - train loss: 0.2523, val loss: 0.2500\n",
      "epoch 29/10000 (batch 80000) - train loss: 0.2478, val loss: 0.2454\n",
      "epoch 29/10000 (batch 81000) - train loss: 0.2477, val loss: 0.2448\n",
      "epoch 29/10000 (batch 82000) - train loss: 0.2462, val loss: 0.2409\n",
      "epoch 30/10000 (batch 83000) - train loss: 0.2404, val loss: 0.2393\n",
      "epoch 30/10000 (batch 84000) - train loss: 0.2396, val loss: 0.2358\n",
      "epoch 30/10000 (batch 85000) - train loss: 0.2383, val loss: 0.2342\n",
      "epoch 31/10000 (batch 86000) - train loss: 0.2363, val loss: 0.2319\n",
      "epoch 31/10000 (batch 87000) - train loss: 0.2323, val loss: 0.2286\n",
      "epoch 31/10000 (batch 88000) - train loss: 0.2299, val loss: 0.2263\n",
      "epoch 32/10000 (batch 89000) - train loss: 0.2236, val loss: 0.2240\n",
      "epoch 32/10000 (batch 90000) - train loss: 0.2242, val loss: 0.2223\n",
      "epoch 32/10000 (batch 91000) - train loss: 0.2230, val loss: 0.2204\n",
      "epoch 33/10000 (batch 92000) - train loss: 0.2199, val loss: 0.2193\n",
      "epoch 33/10000 (batch 93000) - train loss: 0.2183, val loss: 0.2168\n",
      "epoch 33/10000 (batch 94000) - train loss: 0.2173, val loss: 0.2140\n",
      "epoch 34/10000 (batch 95000) - train loss: 0.2144, val loss: 0.2134\n",
      "epoch 34/10000 (batch 96000) - train loss: 0.2135, val loss: 0.2114\n",
      "epoch 35/10000 (batch 97000) - train loss: 0.2072, val loss: 0.2101\n",
      "epoch 35/10000 (batch 98000) - train loss: 0.2103, val loss: 0.2082\n",
      "epoch 35/10000 (batch 99000) - train loss: 0.2092, val loss: 0.2076\n",
      "epoch 36/10000 (batch 100000) - train loss: 0.2070, val loss: 0.2070\n",
      "epoch 36/10000 (batch 101000) - train loss: 0.2059, val loss: 0.2053\n",
      "epoch 36/10000 (batch 102000) - train loss: 0.2066, val loss: 0.2050\n",
      "epoch 37/10000 (batch 103000) - train loss: 0.2043, val loss: 0.2042\n",
      "epoch 37/10000 (batch 104000) - train loss: 0.2057, val loss: 0.2036\n",
      "epoch 37/10000 (batch 105000) - train loss: 0.2041, val loss: 0.2035\n",
      "epoch 38/10000 (batch 106000) - train loss: 0.2027, val loss: 0.2029\n",
      "epoch 38/10000 (batch 107000) - train loss: 0.2021, val loss: 0.2025\n",
      "epoch 38/10000 (batch 108000) - train loss: 0.2024, val loss: 0.2024\n",
      "epoch 39/10000 (batch 109000) - train loss: 0.2020, val loss: 0.2017\n",
      "epoch 39/10000 (batch 110000) - train loss: 0.2024, val loss: 0.2015\n",
      "epoch 39/10000 (batch 111000) - train loss: 0.2016, val loss: 0.2004\n",
      "epoch 40/10000 (batch 112000) - train loss: 0.2020, val loss: 0.2008\n",
      "Early stopping triggered at epoch 40, batch 112000\n",
      "embedding generation completed! :) \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b7b1789104843b2abf690914cbccc3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n",
      "/home/gokul/.local/lib/python3.10/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n",
      "fractions:   0%|          | 0/10 [00:00<?, ?it/s]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/home/gokul/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "Monitored metric elbo_validation did not improve in the last 45 records. Best score: 135.026. Signaling Trainer to stop.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "epoch 500/10000 (batch 1000) - train loss: 2.7696, val loss: 2.5200\n",
      "epoch 1000/10000 (batch 2000) - train loss: 0.4676, val loss: 0.3772\n",
      "epoch 1500/10000 (batch 3000) - train loss: 0.4075, val loss: 0.3219\n",
      "epoch 2000/10000 (batch 4000) - train loss: 0.3960, val loss: 0.2668\n",
      "epoch 2500/10000 (batch 5000) - train loss: 0.3533, val loss: 0.2504\n",
      "epoch 3000/10000 (batch 6000) - train loss: 0.3564, val loss: 0.2765\n",
      "Early stopping triggered at epoch 3000, batch 6000\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/home/gokul/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "Monitored metric elbo_validation did not improve in the last 45 records. Best score: 157.645. Signaling Trainer to stop.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/home/gokul/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=400` reached.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/home/gokul/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "Monitored metric elbo_validation did not improve in the last 45 records. Best score: 142.268. Signaling Trainer to stop.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "epoch 36/10000 (batch 1000) - train loss: 7.4288, val loss: 7.4570\n",
      "epoch 72/10000 (batch 2000) - train loss: 6.3531, val loss: 6.3476\n",
      "epoch 108/10000 (batch 3000) - train loss: 4.5036, val loss: 4.5157\n",
      "epoch 143/10000 (batch 4000) - train loss: 2.1049, val loss: 2.1032\n",
      "epoch 179/10000 (batch 5000) - train loss: 0.7274, val loss: 0.7850\n",
      "epoch 215/10000 (batch 6000) - train loss: 0.5439, val loss: 0.5629\n",
      "epoch 250/10000 (batch 7000) - train loss: 0.4848, val loss: 0.5239\n",
      "epoch 286/10000 (batch 8000) - train loss: 0.4534, val loss: 0.5100\n",
      "epoch 322/10000 (batch 9000) - train loss: 0.4332, val loss: 0.4748\n",
      "epoch 358/10000 (batch 10000) - train loss: 0.4114, val loss: 0.4627\n",
      "epoch 393/10000 (batch 11000) - train loss: 0.3805, val loss: 0.4313\n",
      "epoch 429/10000 (batch 12000) - train loss: 0.3580, val loss: 0.4001\n",
      "epoch 465/10000 (batch 13000) - train loss: 0.3557, val loss: 0.3955\n",
      "epoch 500/10000 (batch 14000) - train loss: 0.3445, val loss: 0.3927\n",
      "epoch 536/10000 (batch 15000) - train loss: 0.3343, val loss: 0.3884\n",
      "epoch 572/10000 (batch 16000) - train loss: 0.3240, val loss: 0.3799\n",
      "epoch 608/10000 (batch 17000) - train loss: 0.3424, val loss: 0.3911\n",
      "Early stopping triggered at epoch 608, batch 17000\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/home/gokul/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=400` reached.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "`Trainer.fit` stopped: `max_epochs=400` reached.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "`Trainer.fit` stopped: `max_epochs=400` reached.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "epoch 4/10000 (batch 1000) - train loss: 6.5944, val loss: 6.5735\n",
      "epoch 8/10000 (batch 2000) - train loss: 6.4587, val loss: 6.4596\n",
      "epoch 11/10000 (batch 3000) - train loss: 6.2881, val loss: 6.2724\n",
      "epoch 15/10000 (batch 4000) - train loss: 6.0119, val loss: 6.0115\n",
      "epoch 18/10000 (batch 5000) - train loss: 5.7033, val loss: 5.6764\n",
      "epoch 22/10000 (batch 6000) - train loss: 5.2679, val loss: 5.2682\n",
      "epoch 25/10000 (batch 7000) - train loss: 4.8228, val loss: 4.7860\n",
      "epoch 29/10000 (batch 8000) - train loss: 4.2286, val loss: 4.2286\n",
      "epoch 32/10000 (batch 9000) - train loss: 3.6532, val loss: 3.6041\n",
      "epoch 36/10000 (batch 10000) - train loss: 2.9376, val loss: 2.9279\n",
      "epoch 39/10000 (batch 11000) - train loss: 2.2773, val loss: 2.2315\n",
      "epoch 43/10000 (batch 12000) - train loss: 1.5889, val loss: 1.5902\n",
      "epoch 46/10000 (batch 13000) - train loss: 1.1198, val loss: 1.1014\n",
      "epoch 50/10000 (batch 14000) - train loss: 0.7869, val loss: 0.8019\n",
      "epoch 53/10000 (batch 15000) - train loss: 0.6373, val loss: 0.6443\n",
      "epoch 57/10000 (batch 16000) - train loss: 0.5422, val loss: 0.5701\n",
      "epoch 60/10000 (batch 17000) - train loss: 0.5114, val loss: 0.5345\n",
      "epoch 64/10000 (batch 18000) - train loss: 0.5008, val loss: 0.5106\n",
      "epoch 67/10000 (batch 19000) - train loss: 0.4789, val loss: 0.4957\n",
      "epoch 71/10000 (batch 20000) - train loss: 0.4531, val loss: 0.4882\n",
      "epoch 74/10000 (batch 21000) - train loss: 0.4605, val loss: 0.4778\n",
      "epoch 78/10000 (batch 22000) - train loss: 0.4503, val loss: 0.4672\n",
      "epoch 81/10000 (batch 23000) - train loss: 0.4475, val loss: 0.4609\n",
      "epoch 85/10000 (batch 24000) - train loss: 0.4341, val loss: 0.4517\n",
      "epoch 88/10000 (batch 25000) - train loss: 0.4272, val loss: 0.4351\n",
      "epoch 92/10000 (batch 26000) - train loss: 0.4044, val loss: 0.4292\n",
      "epoch 95/10000 (batch 27000) - train loss: 0.3994, val loss: 0.4121\n",
      "epoch 99/10000 (batch 28000) - train loss: 0.3909, val loss: 0.4052\n",
      "epoch 102/10000 (batch 29000) - train loss: 0.3783, val loss: 0.3921\n",
      "epoch 106/10000 (batch 30000) - train loss: 0.3725, val loss: 0.3821\n",
      "epoch 109/10000 (batch 31000) - train loss: 0.3612, val loss: 0.3771\n",
      "epoch 113/10000 (batch 32000) - train loss: 0.3550, val loss: 0.3701\n",
      "epoch 116/10000 (batch 33000) - train loss: 0.3537, val loss: 0.3659\n",
      "epoch 120/10000 (batch 34000) - train loss: 0.3399, val loss: 0.3643\n",
      "epoch 123/10000 (batch 35000) - train loss: 0.3489, val loss: 0.3650\n",
      "Early stopping triggered at epoch 123, batch 35000\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "`Trainer.fit` stopped: `max_epochs=326` reached.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "`Trainer.fit` stopped: `max_epochs=151` reached.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "`Trainer.fit` stopped: `max_epochs=70` reached.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "epoch 1/10000 (batch 1000) - train loss: 7.0199, val loss: 7.0290\n",
      "epoch 1/10000 (batch 2000) - train loss: 7.0218, val loss: 7.0173\n",
      "epoch 2/10000 (batch 3000) - train loss: 6.9997, val loss: 6.9982\n",
      "epoch 2/10000 (batch 4000) - train loss: 6.9872, val loss: 6.9719\n",
      "epoch 2/10000 (batch 5000) - train loss: 6.9724, val loss: 6.9380\n",
      "epoch 3/10000 (batch 6000) - train loss: 6.9027, val loss: 6.8962\n",
      "epoch 3/10000 (batch 7000) - train loss: 6.8791, val loss: 6.8470\n",
      "epoch 3/10000 (batch 8000) - train loss: 6.8531, val loss: 6.7907\n",
      "epoch 4/10000 (batch 9000) - train loss: 6.7412, val loss: 6.7263\n",
      "epoch 4/10000 (batch 10000) - train loss: 6.7066, val loss: 6.6541\n",
      "epoch 4/10000 (batch 11000) - train loss: 6.6694, val loss: 6.5752\n",
      "epoch 5/10000 (batch 12000) - train loss: 6.5151, val loss: 6.4885\n",
      "epoch 5/10000 (batch 13000) - train loss: 6.4692, val loss: 6.3942\n",
      "epoch 5/10000 (batch 14000) - train loss: 6.4209, val loss: 6.2921\n",
      "epoch 6/10000 (batch 15000) - train loss: 6.2245, val loss: 6.1829\n",
      "epoch 6/10000 (batch 16000) - train loss: 6.1677, val loss: 6.0656\n",
      "epoch 6/10000 (batch 17000) - train loss: 6.1081, val loss: 5.9411\n",
      "epoch 7/10000 (batch 18000) - train loss: 5.8686, val loss: 5.8091\n",
      "epoch 7/10000 (batch 19000) - train loss: 5.8005, val loss: 5.6690\n",
      "epoch 8/10000 (batch 20000) - train loss: 5.5243, val loss: 5.5219\n",
      "epoch 8/10000 (batch 21000) - train loss: 5.4484, val loss: 5.3671\n",
      "epoch 8/10000 (batch 22000) - train loss: 5.3695, val loss: 5.2045\n",
      "epoch 9/10000 (batch 23000) - train loss: 5.0517, val loss: 5.0345\n",
      "epoch 9/10000 (batch 24000) - train loss: 4.9632, val loss: 4.8566\n",
      "epoch 9/10000 (batch 25000) - train loss: 4.8728, val loss: 4.6712\n",
      "epoch 10/10000 (batch 26000) - train loss: 4.5126, val loss: 4.4787\n",
      "epoch 10/10000 (batch 27000) - train loss: 4.4131, val loss: 4.2776\n",
      "epoch 10/10000 (batch 28000) - train loss: 4.3115, val loss: 4.0704\n",
      "epoch 11/10000 (batch 29000) - train loss: 3.9097, val loss: 3.8560\n",
      "epoch 11/10000 (batch 30000) - train loss: 3.8004, val loss: 3.6351\n",
      "epoch 11/10000 (batch 31000) - train loss: 3.6892, val loss: 3.4079\n",
      "epoch 12/10000 (batch 32000) - train loss: 3.2511, val loss: 3.1760\n",
      "epoch 12/10000 (batch 33000) - train loss: 3.1339, val loss: 2.9395\n",
      "epoch 12/10000 (batch 34000) - train loss: 3.0154, val loss: 2.7003\n",
      "epoch 13/10000 (batch 35000) - train loss: 2.5568, val loss: 2.4608\n",
      "epoch 13/10000 (batch 36000) - train loss: 2.4366, val loss: 2.2222\n",
      "epoch 13/10000 (batch 37000) - train loss: 2.3182, val loss: 1.9899\n",
      "epoch 14/10000 (batch 38000) - train loss: 1.8689, val loss: 1.7643\n",
      "epoch 14/10000 (batch 39000) - train loss: 1.7602, val loss: 1.5525\n",
      "epoch 15/10000 (batch 40000) - train loss: 1.3586, val loss: 1.3573\n",
      "epoch 15/10000 (batch 41000) - train loss: 1.2776, val loss: 1.1842\n",
      "epoch 15/10000 (batch 42000) - train loss: 1.1961, val loss: 1.0318\n",
      "epoch 16/10000 (batch 43000) - train loss: 0.9252, val loss: 0.9053\n",
      "epoch 16/10000 (batch 44000) - train loss: 0.8668, val loss: 0.8006\n",
      "epoch 16/10000 (batch 45000) - train loss: 0.8186, val loss: 0.7179\n",
      "epoch 17/10000 (batch 46000) - train loss: 0.6669, val loss: 0.6551\n",
      "epoch 17/10000 (batch 47000) - train loss: 0.6418, val loss: 0.6082\n",
      "epoch 17/10000 (batch 48000) - train loss: 0.6199, val loss: 0.5707\n",
      "epoch 18/10000 (batch 49000) - train loss: 0.5504, val loss: 0.5443\n",
      "epoch 18/10000 (batch 50000) - train loss: 0.5431, val loss: 0.5246\n",
      "epoch 18/10000 (batch 51000) - train loss: 0.5346, val loss: 0.5102\n",
      "epoch 19/10000 (batch 52000) - train loss: 0.5080, val loss: 0.5023\n",
      "epoch 19/10000 (batch 53000) - train loss: 0.5023, val loss: 0.4948\n",
      "epoch 19/10000 (batch 54000) - train loss: 0.4981, val loss: 0.4900\n",
      "epoch 20/10000 (batch 55000) - train loss: 0.4851, val loss: 0.4833\n",
      "epoch 20/10000 (batch 56000) - train loss: 0.4844, val loss: 0.4821\n",
      "epoch 20/10000 (batch 57000) - train loss: 0.4847, val loss: 0.4785\n",
      "epoch 21/10000 (batch 58000) - train loss: 0.4810, val loss: 0.4733\n",
      "epoch 21/10000 (batch 59000) - train loss: 0.4777, val loss: 0.4716\n",
      "epoch 22/10000 (batch 60000) - train loss: 0.4679, val loss: 0.4684\n",
      "epoch 22/10000 (batch 61000) - train loss: 0.4682, val loss: 0.4654\n",
      "epoch 22/10000 (batch 62000) - train loss: 0.4679, val loss: 0.4647\n",
      "epoch 23/10000 (batch 63000) - train loss: 0.4659, val loss: 0.4603\n",
      "epoch 23/10000 (batch 64000) - train loss: 0.4613, val loss: 0.4569\n",
      "epoch 23/10000 (batch 65000) - train loss: 0.4619, val loss: 0.4566\n",
      "epoch 24/10000 (batch 66000) - train loss: 0.4551, val loss: 0.4518\n",
      "epoch 24/10000 (batch 67000) - train loss: 0.4539, val loss: 0.4497\n",
      "epoch 24/10000 (batch 68000) - train loss: 0.4540, val loss: 0.4483\n",
      "epoch 25/10000 (batch 69000) - train loss: 0.4434, val loss: 0.4454\n",
      "epoch 25/10000 (batch 70000) - train loss: 0.4450, val loss: 0.4437\n",
      "epoch 25/10000 (batch 71000) - train loss: 0.4436, val loss: 0.4403\n",
      "epoch 26/10000 (batch 72000) - train loss: 0.4370, val loss: 0.4353\n",
      "epoch 26/10000 (batch 73000) - train loss: 0.4355, val loss: 0.4321\n",
      "epoch 26/10000 (batch 74000) - train loss: 0.4352, val loss: 0.4292\n",
      "epoch 27/10000 (batch 75000) - train loss: 0.4282, val loss: 0.4262\n",
      "epoch 27/10000 (batch 76000) - train loss: 0.4285, val loss: 0.4215\n",
      "epoch 28/10000 (batch 77000) - train loss: 0.4123, val loss: 0.4188\n",
      "epoch 28/10000 (batch 78000) - train loss: 0.4191, val loss: 0.4160\n",
      "epoch 28/10000 (batch 79000) - train loss: 0.4167, val loss: 0.4127\n",
      "epoch 29/10000 (batch 80000) - train loss: 0.4137, val loss: 0.4084\n",
      "epoch 29/10000 (batch 81000) - train loss: 0.4096, val loss: 0.4032\n",
      "epoch 29/10000 (batch 82000) - train loss: 0.4075, val loss: 0.4012\n",
      "epoch 30/10000 (batch 83000) - train loss: 0.3955, val loss: 0.3974\n",
      "epoch 30/10000 (batch 84000) - train loss: 0.3980, val loss: 0.3929\n",
      "epoch 30/10000 (batch 85000) - train loss: 0.3967, val loss: 0.3915\n",
      "epoch 31/10000 (batch 86000) - train loss: 0.3900, val loss: 0.3860\n",
      "epoch 31/10000 (batch 87000) - train loss: 0.3870, val loss: 0.3846\n",
      "epoch 31/10000 (batch 88000) - train loss: 0.3859, val loss: 0.3806\n",
      "epoch 32/10000 (batch 89000) - train loss: 0.3798, val loss: 0.3782\n",
      "epoch 32/10000 (batch 90000) - train loss: 0.3792, val loss: 0.3751\n",
      "epoch 32/10000 (batch 91000) - train loss: 0.3774, val loss: 0.3723\n",
      "epoch 33/10000 (batch 92000) - train loss: 0.3726, val loss: 0.3700\n",
      "epoch 33/10000 (batch 93000) - train loss: 0.3715, val loss: 0.3671\n",
      "epoch 33/10000 (batch 94000) - train loss: 0.3701, val loss: 0.3665\n",
      "epoch 34/10000 (batch 95000) - train loss: 0.3657, val loss: 0.3631\n",
      "epoch 34/10000 (batch 96000) - train loss: 0.3648, val loss: 0.3624\n",
      "epoch 35/10000 (batch 97000) - train loss: 0.3614, val loss: 0.3593\n",
      "epoch 35/10000 (batch 98000) - train loss: 0.3591, val loss: 0.3572\n",
      "epoch 35/10000 (batch 99000) - train loss: 0.3586, val loss: 0.3581\n",
      "Early stopping triggered at epoch 35, batch 99000\n",
      "embedding generation completed! :) \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "813769e09a5543fc92d190b213df5b7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n",
      "/home/gokul/.local/lib/python3.10/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n",
      "fractions:   0%|          | 0/10 [00:00<?, ?it/s]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/home/gokul/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "Monitored metric elbo_validation did not improve in the last 45 records. Best score: 215.532. Signaling Trainer to stop.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "epoch 500/10000 (batch 1000) - train loss: 1.8636, val loss: 1.5686\n",
      "epoch 1000/10000 (batch 2000) - train loss: 0.6845, val loss: 0.6238\n",
      "epoch 1500/10000 (batch 3000) - train loss: 0.6569, val loss: 0.6046\n",
      "epoch 2000/10000 (batch 4000) - train loss: 0.5278, val loss: 0.4692\n",
      "epoch 2500/10000 (batch 5000) - train loss: 0.4806, val loss: 0.4639\n",
      "epoch 3000/10000 (batch 6000) - train loss: 0.4560, val loss: 0.4872\n",
      "Early stopping triggered at epoch 3000, batch 6000\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/home/gokul/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "Monitored metric elbo_validation did not improve in the last 45 records. Best score: 202.976. Signaling Trainer to stop.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/home/gokul/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=400` reached.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/home/gokul/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "Monitored metric elbo_validation did not improve in the last 45 records. Best score: 195.322. Signaling Trainer to stop.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "epoch 36/10000 (batch 1000) - train loss: 6.0649, val loss: 6.0159\n",
      "epoch 72/10000 (batch 2000) - train loss: 4.8639, val loss: 4.8555\n",
      "epoch 108/10000 (batch 3000) - train loss: 2.9933, val loss: 2.9901\n",
      "epoch 143/10000 (batch 4000) - train loss: 1.2587, val loss: 1.2489\n",
      "epoch 179/10000 (batch 5000) - train loss: 0.7787, val loss: 0.7901\n",
      "epoch 215/10000 (batch 6000) - train loss: 0.7103, val loss: 0.6960\n",
      "epoch 250/10000 (batch 7000) - train loss: 0.6794, val loss: 0.6903\n",
      "epoch 286/10000 (batch 8000) - train loss: 0.6496, val loss: 0.6545\n",
      "epoch 322/10000 (batch 9000) - train loss: 0.6309, val loss: 0.6144\n",
      "epoch 358/10000 (batch 10000) - train loss: 0.5702, val loss: 0.6008\n",
      "epoch 393/10000 (batch 11000) - train loss: 0.5628, val loss: 0.5509\n",
      "epoch 429/10000 (batch 12000) - train loss: 0.5348, val loss: 0.5400\n",
      "epoch 465/10000 (batch 13000) - train loss: 0.5288, val loss: 0.5390\n",
      "epoch 500/10000 (batch 14000) - train loss: 0.5248, val loss: 0.5217\n",
      "epoch 536/10000 (batch 15000) - train loss: 0.5038, val loss: 0.5235\n",
      "Early stopping triggered at epoch 536, batch 15000\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/home/gokul/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "Monitored metric elbo_validation did not improve in the last 45 records. Best score: 192.860. Signaling Trainer to stop.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "`Trainer.fit` stopped: `max_epochs=400` reached.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "`Trainer.fit` stopped: `max_epochs=400` reached.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "epoch 4/10000 (batch 1000) - train loss: 6.8287, val loss: 6.8280\n",
      "epoch 8/10000 (batch 2000) - train loss: 6.7135, val loss: 6.7161\n",
      "epoch 11/10000 (batch 3000) - train loss: 6.5469, val loss: 6.5310\n",
      "epoch 15/10000 (batch 4000) - train loss: 6.2755, val loss: 6.2738\n",
      "epoch 18/10000 (batch 5000) - train loss: 5.9702, val loss: 5.9438\n",
      "epoch 22/10000 (batch 6000) - train loss: 5.5369, val loss: 5.5381\n",
      "epoch 25/10000 (batch 7000) - train loss: 5.0972, val loss: 5.0608\n",
      "epoch 29/10000 (batch 8000) - train loss: 4.5157, val loss: 4.5114\n",
      "epoch 32/10000 (batch 9000) - train loss: 3.9300, val loss: 3.8840\n",
      "epoch 36/10000 (batch 10000) - train loss: 3.2026, val loss: 3.2061\n",
      "epoch 39/10000 (batch 11000) - train loss: 2.5440, val loss: 2.4961\n",
      "epoch 43/10000 (batch 12000) - train loss: 1.8324, val loss: 1.8317\n",
      "epoch 46/10000 (batch 13000) - train loss: 1.3420, val loss: 1.3243\n",
      "epoch 50/10000 (batch 14000) - train loss: 0.9948, val loss: 1.0126\n",
      "epoch 53/10000 (batch 15000) - train loss: 0.8473, val loss: 0.8475\n",
      "epoch 57/10000 (batch 16000) - train loss: 0.7809, val loss: 0.7818\n",
      "epoch 60/10000 (batch 17000) - train loss: 0.7276, val loss: 0.7509\n",
      "epoch 64/10000 (batch 18000) - train loss: 0.7073, val loss: 0.7332\n",
      "epoch 67/10000 (batch 19000) - train loss: 0.6990, val loss: 0.7238\n",
      "epoch 71/10000 (batch 20000) - train loss: 0.7031, val loss: 0.7008\n",
      "epoch 74/10000 (batch 21000) - train loss: 0.6772, val loss: 0.6935\n",
      "epoch 78/10000 (batch 22000) - train loss: 0.6727, val loss: 0.6836\n",
      "epoch 81/10000 (batch 23000) - train loss: 0.6562, val loss: 0.6722\n",
      "epoch 85/10000 (batch 24000) - train loss: 0.6425, val loss: 0.6585\n",
      "epoch 88/10000 (batch 25000) - train loss: 0.6249, val loss: 0.6485\n",
      "epoch 92/10000 (batch 26000) - train loss: 0.5994, val loss: 0.6363\n",
      "epoch 95/10000 (batch 27000) - train loss: 0.5998, val loss: 0.6132\n",
      "epoch 99/10000 (batch 28000) - train loss: 0.5858, val loss: 0.6102\n",
      "epoch 102/10000 (batch 29000) - train loss: 0.5722, val loss: 0.5933\n",
      "epoch 106/10000 (batch 30000) - train loss: 0.5451, val loss: 0.5819\n",
      "epoch 109/10000 (batch 31000) - train loss: 0.5493, val loss: 0.5651\n",
      "epoch 113/10000 (batch 32000) - train loss: 0.5405, val loss: 0.5647\n",
      "epoch 116/10000 (batch 33000) - train loss: 0.5370, val loss: 0.5551\n",
      "epoch 120/10000 (batch 34000) - train loss: 0.5479, val loss: 0.5572\n",
      "Early stopping triggered at epoch 120, batch 34000\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "`Trainer.fit` stopped: `max_epochs=326` reached.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "`Trainer.fit` stopped: `max_epochs=151` reached.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "`Trainer.fit` stopped: `max_epochs=70` reached.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "epoch 1/10000 (batch 1000) - train loss: 5.9791, val loss: 5.9350\n",
      "epoch 1/10000 (batch 2000) - train loss: 5.9545, val loss: 5.9233\n",
      "epoch 2/10000 (batch 3000) - train loss: 5.9058, val loss: 5.9054\n",
      "epoch 2/10000 (batch 4000) - train loss: 5.8955, val loss: 5.8799\n",
      "epoch 2/10000 (batch 5000) - train loss: 5.8813, val loss: 5.8476\n",
      "epoch 3/10000 (batch 6000) - train loss: 5.8150, val loss: 5.8079\n",
      "epoch 3/10000 (batch 7000) - train loss: 5.7919, val loss: 5.7608\n",
      "epoch 3/10000 (batch 8000) - train loss: 5.7672, val loss: 5.7072\n",
      "epoch 4/10000 (batch 9000) - train loss: 5.6615, val loss: 5.6460\n",
      "epoch 4/10000 (batch 10000) - train loss: 5.6273, val loss: 5.5779\n",
      "epoch 4/10000 (batch 11000) - train loss: 5.5919, val loss: 5.5018\n",
      "epoch 5/10000 (batch 12000) - train loss: 5.4439, val loss: 5.4196\n",
      "epoch 5/10000 (batch 13000) - train loss: 5.4008, val loss: 5.3290\n",
      "epoch 5/10000 (batch 14000) - train loss: 5.3549, val loss: 5.2323\n",
      "epoch 6/10000 (batch 15000) - train loss: 5.1674, val loss: 5.1277\n",
      "epoch 6/10000 (batch 16000) - train loss: 5.1132, val loss: 5.0166\n",
      "epoch 6/10000 (batch 17000) - train loss: 5.0569, val loss: 4.8981\n",
      "epoch 7/10000 (batch 18000) - train loss: 4.8283, val loss: 4.7724\n",
      "epoch 7/10000 (batch 19000) - train loss: 4.7645, val loss: 4.6385\n",
      "epoch 8/10000 (batch 20000) - train loss: 4.5005, val loss: 4.4990\n",
      "epoch 8/10000 (batch 21000) - train loss: 4.4318, val loss: 4.3528\n",
      "epoch 8/10000 (batch 22000) - train loss: 4.3554, val loss: 4.1982\n",
      "epoch 9/10000 (batch 23000) - train loss: 4.0532, val loss: 4.0371\n",
      "epoch 9/10000 (batch 24000) - train loss: 3.9711, val loss: 3.8695\n",
      "epoch 9/10000 (batch 25000) - train loss: 3.8869, val loss: 3.6981\n",
      "epoch 10/10000 (batch 26000) - train loss: 3.5499, val loss: 3.5178\n",
      "epoch 10/10000 (batch 27000) - train loss: 3.4573, val loss: 3.3320\n",
      "epoch 10/10000 (batch 28000) - train loss: 3.3640, val loss: 3.1424\n",
      "epoch 11/10000 (batch 29000) - train loss: 2.9942, val loss: 2.9486\n",
      "epoch 11/10000 (batch 30000) - train loss: 2.8972, val loss: 2.7511\n",
      "epoch 11/10000 (batch 31000) - train loss: 2.7988, val loss: 2.5497\n",
      "epoch 12/10000 (batch 32000) - train loss: 2.4160, val loss: 2.3529\n",
      "epoch 12/10000 (batch 33000) - train loss: 2.3182, val loss: 2.1562\n",
      "epoch 12/10000 (batch 34000) - train loss: 2.2197, val loss: 1.9626\n",
      "epoch 13/10000 (batch 35000) - train loss: 1.8525, val loss: 1.7806\n",
      "epoch 13/10000 (batch 36000) - train loss: 1.7628, val loss: 1.6053\n",
      "epoch 13/10000 (batch 37000) - train loss: 1.6783, val loss: 1.4454\n",
      "epoch 14/10000 (batch 38000) - train loss: 1.3691, val loss: 1.3038\n",
      "epoch 14/10000 (batch 39000) - train loss: 1.3025, val loss: 1.1797\n",
      "epoch 15/10000 (batch 40000) - train loss: 1.0821, val loss: 1.0735\n",
      "epoch 15/10000 (batch 41000) - train loss: 1.0356, val loss: 0.9852\n",
      "epoch 15/10000 (batch 42000) - train loss: 0.9946, val loss: 0.9174\n",
      "epoch 16/10000 (batch 43000) - train loss: 0.8811, val loss: 0.8614\n",
      "epoch 16/10000 (batch 44000) - train loss: 0.8461, val loss: 0.8167\n",
      "epoch 16/10000 (batch 45000) - train loss: 0.8258, val loss: 0.7851\n",
      "epoch 17/10000 (batch 46000) - train loss: 0.7620, val loss: 0.7601\n",
      "epoch 17/10000 (batch 47000) - train loss: 0.7569, val loss: 0.7440\n",
      "epoch 17/10000 (batch 48000) - train loss: 0.7510, val loss: 0.7326\n",
      "epoch 18/10000 (batch 49000) - train loss: 0.7308, val loss: 0.7261\n",
      "epoch 18/10000 (batch 50000) - train loss: 0.7245, val loss: 0.7170\n",
      "epoch 18/10000 (batch 51000) - train loss: 0.7210, val loss: 0.7086\n",
      "epoch 19/10000 (batch 52000) - train loss: 0.7102, val loss: 0.7069\n",
      "epoch 19/10000 (batch 53000) - train loss: 0.7060, val loss: 0.6999\n",
      "epoch 19/10000 (batch 54000) - train loss: 0.7044, val loss: 0.6960\n",
      "epoch 20/10000 (batch 55000) - train loss: 0.6983, val loss: 0.6942\n",
      "epoch 20/10000 (batch 56000) - train loss: 0.6955, val loss: 0.6926\n",
      "epoch 20/10000 (batch 57000) - train loss: 0.6942, val loss: 0.6893\n",
      "epoch 21/10000 (batch 58000) - train loss: 0.6853, val loss: 0.6850\n",
      "epoch 21/10000 (batch 59000) - train loss: 0.6860, val loss: 0.6846\n",
      "epoch 22/10000 (batch 60000) - train loss: 0.7001, val loss: 0.6787\n",
      "epoch 22/10000 (batch 61000) - train loss: 0.6796, val loss: 0.6772\n",
      "epoch 22/10000 (batch 62000) - train loss: 0.6786, val loss: 0.6722\n",
      "epoch 23/10000 (batch 63000) - train loss: 0.6776, val loss: 0.6704\n",
      "epoch 23/10000 (batch 64000) - train loss: 0.6724, val loss: 0.6679\n",
      "epoch 23/10000 (batch 65000) - train loss: 0.6695, val loss: 0.6644\n",
      "epoch 24/10000 (batch 66000) - train loss: 0.6666, val loss: 0.6613\n",
      "epoch 24/10000 (batch 67000) - train loss: 0.6620, val loss: 0.6592\n",
      "epoch 24/10000 (batch 68000) - train loss: 0.6615, val loss: 0.6519\n",
      "epoch 25/10000 (batch 69000) - train loss: 0.6531, val loss: 0.6514\n",
      "epoch 25/10000 (batch 70000) - train loss: 0.6523, val loss: 0.6491\n",
      "epoch 25/10000 (batch 71000) - train loss: 0.6499, val loss: 0.6443\n",
      "epoch 26/10000 (batch 72000) - train loss: 0.6444, val loss: 0.6419\n",
      "epoch 26/10000 (batch 73000) - train loss: 0.6450, val loss: 0.6384\n",
      "epoch 26/10000 (batch 74000) - train loss: 0.6410, val loss: 0.6313\n",
      "epoch 27/10000 (batch 75000) - train loss: 0.6330, val loss: 0.6279\n",
      "epoch 27/10000 (batch 76000) - train loss: 0.6290, val loss: 0.6242\n",
      "epoch 28/10000 (batch 77000) - train loss: 0.6111, val loss: 0.6198\n",
      "epoch 28/10000 (batch 78000) - train loss: 0.6197, val loss: 0.6160\n",
      "epoch 28/10000 (batch 79000) - train loss: 0.6167, val loss: 0.6124\n",
      "epoch 29/10000 (batch 80000) - train loss: 0.5970, val loss: 0.6068\n",
      "epoch 29/10000 (batch 81000) - train loss: 0.6061, val loss: 0.6041\n",
      "epoch 29/10000 (batch 82000) - train loss: 0.6049, val loss: 0.5985\n",
      "epoch 30/10000 (batch 83000) - train loss: 0.5973, val loss: 0.5941\n",
      "epoch 30/10000 (batch 84000) - train loss: 0.5954, val loss: 0.5873\n",
      "epoch 30/10000 (batch 85000) - train loss: 0.5919, val loss: 0.5865\n",
      "epoch 31/10000 (batch 86000) - train loss: 0.5827, val loss: 0.5820\n",
      "epoch 31/10000 (batch 87000) - train loss: 0.5807, val loss: 0.5777\n",
      "epoch 31/10000 (batch 88000) - train loss: 0.5799, val loss: 0.5720\n",
      "epoch 32/10000 (batch 89000) - train loss: 0.5776, val loss: 0.5695\n",
      "epoch 32/10000 (batch 90000) - train loss: 0.5717, val loss: 0.5651\n",
      "epoch 32/10000 (batch 91000) - train loss: 0.5693, val loss: 0.5620\n",
      "epoch 33/10000 (batch 92000) - train loss: 0.5663, val loss: 0.5599\n",
      "epoch 33/10000 (batch 93000) - train loss: 0.5618, val loss: 0.5565\n",
      "epoch 33/10000 (batch 94000) - train loss: 0.5600, val loss: 0.5531\n",
      "epoch 34/10000 (batch 95000) - train loss: 0.5538, val loss: 0.5533\n",
      "Early stopping triggered at epoch 34, batch 95000\n",
      "embedding generation completed! :) \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba9acc0cd97249e3a728f62294b05d11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n",
      "/home/gokul/.local/lib/python3.10/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n",
      "fractions:   0%|          | 0/10 [00:00<?, ?it/s]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/home/gokul/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "Monitored metric elbo_validation did not improve in the last 45 records. Best score: 297.211. Signaling Trainer to stop.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "epoch 500/10000 (batch 1000) - train loss: 1.6406, val loss: 1.5025\n",
      "epoch 1000/10000 (batch 2000) - train loss: 0.9029, val loss: 0.8341\n",
      "epoch 1500/10000 (batch 3000) - train loss: 0.7849, val loss: 0.7163\n",
      "epoch 2000/10000 (batch 4000) - train loss: 0.6916, val loss: 0.6528\n",
      "epoch 2500/10000 (batch 5000) - train loss: 0.7119, val loss: 0.7012\n",
      "Early stopping triggered at epoch 2500, batch 5000\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/home/gokul/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "Monitored metric elbo_validation did not improve in the last 45 records. Best score: 289.026. Signaling Trainer to stop.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/home/gokul/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "Monitored metric elbo_validation did not improve in the last 45 records. Best score: 347.111. Signaling Trainer to stop.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/home/gokul/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=400` reached.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "epoch 36/10000 (batch 1000) - train loss: 6.4998, val loss: 6.4785\n",
      "epoch 72/10000 (batch 2000) - train loss: 5.4042, val loss: 5.3969\n",
      "epoch 108/10000 (batch 3000) - train loss: 3.6234, val loss: 3.6495\n",
      "epoch 143/10000 (batch 4000) - train loss: 1.7151, val loss: 1.7057\n",
      "epoch 179/10000 (batch 5000) - train loss: 1.0626, val loss: 1.0518\n",
      "epoch 215/10000 (batch 6000) - train loss: 1.0139, val loss: 0.9834\n",
      "epoch 250/10000 (batch 7000) - train loss: 0.9395, val loss: 0.9196\n",
      "epoch 286/10000 (batch 8000) - train loss: 0.9024, val loss: 0.9046\n",
      "epoch 322/10000 (batch 9000) - train loss: 0.8315, val loss: 0.8711\n",
      "epoch 358/10000 (batch 10000) - train loss: 0.7257, val loss: 0.8206\n",
      "epoch 393/10000 (batch 11000) - train loss: 0.7773, val loss: 0.7835\n",
      "epoch 429/10000 (batch 12000) - train loss: 0.7667, val loss: 0.7662\n",
      "epoch 465/10000 (batch 13000) - train loss: 0.7615, val loss: 0.7488\n",
      "epoch 500/10000 (batch 14000) - train loss: 0.7353, val loss: 0.7487\n",
      "epoch 536/10000 (batch 15000) - train loss: 0.7572, val loss: 0.7471\n",
      "epoch 572/10000 (batch 16000) - train loss: 0.7257, val loss: 0.7406\n",
      "epoch 608/10000 (batch 17000) - train loss: 0.7484, val loss: 0.7379\n",
      "epoch 643/10000 (batch 18000) - train loss: 0.7284, val loss: 0.7336\n",
      "epoch 679/10000 (batch 19000) - train loss: 0.7443, val loss: 0.7585\n",
      "Early stopping triggered at epoch 679, batch 19000\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/home/gokul/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=400` reached.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "`Trainer.fit` stopped: `max_epochs=400` reached.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "`Trainer.fit` stopped: `max_epochs=400` reached.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "epoch 4/10000 (batch 1000) - train loss: 6.5832, val loss: 6.5637\n",
      "epoch 8/10000 (batch 2000) - train loss: 6.4555, val loss: 6.4546\n",
      "epoch 11/10000 (batch 3000) - train loss: 6.2945, val loss: 6.2763\n",
      "epoch 15/10000 (batch 4000) - train loss: 6.0325, val loss: 6.0289\n",
      "epoch 18/10000 (batch 5000) - train loss: 5.7404, val loss: 5.7098\n",
      "epoch 22/10000 (batch 6000) - train loss: 5.3355, val loss: 5.3201\n",
      "epoch 25/10000 (batch 7000) - train loss: 4.9072, val loss: 4.8602\n",
      "epoch 29/10000 (batch 8000) - train loss: 4.3473, val loss: 4.3335\n",
      "epoch 32/10000 (batch 9000) - train loss: 3.8042, val loss: 3.7393\n",
      "epoch 36/10000 (batch 10000) - train loss: 3.1172, val loss: 3.0935\n",
      "epoch 39/10000 (batch 11000) - train loss: 2.5126, val loss: 2.4409\n",
      "epoch 43/10000 (batch 12000) - train loss: 1.8746, val loss: 1.8484\n",
      "epoch 46/10000 (batch 13000) - train loss: 1.4666, val loss: 1.4154\n",
      "epoch 50/10000 (batch 14000) - train loss: 1.1755, val loss: 1.1620\n",
      "epoch 53/10000 (batch 15000) - train loss: 1.0698, val loss: 1.0336\n",
      "epoch 57/10000 (batch 16000) - train loss: 0.9723, val loss: 0.9717\n",
      "epoch 60/10000 (batch 17000) - train loss: 0.9671, val loss: 0.9376\n",
      "epoch 64/10000 (batch 18000) - train loss: 0.9708, val loss: 0.9264\n",
      "epoch 67/10000 (batch 19000) - train loss: 0.9359, val loss: 0.9143\n",
      "epoch 71/10000 (batch 20000) - train loss: 0.9356, val loss: 0.8908\n",
      "epoch 74/10000 (batch 21000) - train loss: 0.9082, val loss: 0.8795\n",
      "epoch 78/10000 (batch 22000) - train loss: 0.8954, val loss: 0.8651\n",
      "epoch 81/10000 (batch 23000) - train loss: 0.8815, val loss: 0.8553\n",
      "epoch 85/10000 (batch 24000) - train loss: 0.8514, val loss: 0.8330\n",
      "epoch 88/10000 (batch 25000) - train loss: 0.8426, val loss: 0.8247\n",
      "epoch 92/10000 (batch 26000) - train loss: 0.8261, val loss: 0.7971\n",
      "epoch 95/10000 (batch 27000) - train loss: 0.8093, val loss: 0.7870\n",
      "epoch 99/10000 (batch 28000) - train loss: 0.7890, val loss: 0.7726\n",
      "epoch 102/10000 (batch 29000) - train loss: 0.7817, val loss: 0.7604\n",
      "epoch 106/10000 (batch 30000) - train loss: 0.7622, val loss: 0.7441\n",
      "epoch 109/10000 (batch 31000) - train loss: 0.7593, val loss: 0.7343\n",
      "epoch 113/10000 (batch 32000) - train loss: 0.7630, val loss: 0.7304\n",
      "epoch 116/10000 (batch 33000) - train loss: 0.7448, val loss: 0.7267\n",
      "epoch 120/10000 (batch 34000) - train loss: 0.7525, val loss: 0.7107\n",
      "epoch 123/10000 (batch 35000) - train loss: 0.7382, val loss: 0.7213\n",
      "Early stopping triggered at epoch 123, batch 35000\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "`Trainer.fit` stopped: `max_epochs=326` reached.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "`Trainer.fit` stopped: `max_epochs=151` reached.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "`Trainer.fit` stopped: `max_epochs=70` reached.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "epoch 1/10000 (batch 1000) - train loss: 6.4947, val loss: 6.4708\n",
      "epoch 1/10000 (batch 2000) - train loss: 6.4803, val loss: 6.4602\n",
      "epoch 2/10000 (batch 3000) - train loss: 6.4444, val loss: 6.4420\n",
      "epoch 2/10000 (batch 4000) - train loss: 6.4322, val loss: 6.4173\n",
      "epoch 2/10000 (batch 5000) - train loss: 6.4182, val loss: 6.3853\n",
      "epoch 3/10000 (batch 6000) - train loss: 6.3532, val loss: 6.3465\n",
      "epoch 3/10000 (batch 7000) - train loss: 6.3305, val loss: 6.2999\n",
      "epoch 3/10000 (batch 8000) - train loss: 6.3061, val loss: 6.2474\n",
      "epoch 4/10000 (batch 9000) - train loss: 6.2012, val loss: 6.1870\n",
      "epoch 4/10000 (batch 10000) - train loss: 6.1688, val loss: 6.1197\n",
      "epoch 4/10000 (batch 11000) - train loss: 6.1342, val loss: 6.0459\n",
      "epoch 5/10000 (batch 12000) - train loss: 5.9899, val loss: 5.9651\n",
      "epoch 5/10000 (batch 13000) - train loss: 5.9470, val loss: 5.8765\n",
      "epoch 5/10000 (batch 14000) - train loss: 5.9019, val loss: 5.7810\n",
      "epoch 6/10000 (batch 15000) - train loss: 5.7181, val loss: 5.6787\n",
      "epoch 6/10000 (batch 16000) - train loss: 5.6650, val loss: 5.5694\n",
      "epoch 6/10000 (batch 17000) - train loss: 5.6093, val loss: 5.4527\n",
      "epoch 7/10000 (batch 18000) - train loss: 5.3863, val loss: 5.3290\n",
      "epoch 7/10000 (batch 19000) - train loss: 5.3224, val loss: 5.1989\n",
      "epoch 8/10000 (batch 20000) - train loss: 5.0680, val loss: 5.0607\n",
      "epoch 8/10000 (batch 21000) - train loss: 4.9941, val loss: 4.9167\n",
      "epoch 8/10000 (batch 22000) - train loss: 4.9198, val loss: 4.7659\n",
      "epoch 9/10000 (batch 23000) - train loss: 4.6274, val loss: 4.6072\n",
      "epoch 9/10000 (batch 24000) - train loss: 4.5422, val loss: 4.4423\n",
      "epoch 9/10000 (batch 25000) - train loss: 4.4586, val loss: 4.2701\n",
      "epoch 10/10000 (batch 26000) - train loss: 4.1201, val loss: 4.0919\n",
      "epoch 10/10000 (batch 27000) - train loss: 4.0335, val loss: 3.9067\n",
      "epoch 10/10000 (batch 28000) - train loss: 3.9392, val loss: 3.7177\n",
      "epoch 11/10000 (batch 29000) - train loss: 3.5708, val loss: 3.5218\n",
      "epoch 11/10000 (batch 30000) - train loss: 3.4715, val loss: 3.3215\n",
      "epoch 11/10000 (batch 31000) - train loss: 3.3718, val loss: 3.1174\n",
      "epoch 12/10000 (batch 32000) - train loss: 2.9773, val loss: 2.9107\n",
      "epoch 12/10000 (batch 33000) - train loss: 2.8741, val loss: 2.7045\n",
      "epoch 12/10000 (batch 34000) - train loss: 2.7715, val loss: 2.4975\n",
      "epoch 13/10000 (batch 35000) - train loss: 2.3772, val loss: 2.2965\n",
      "epoch 13/10000 (batch 36000) - train loss: 2.2770, val loss: 2.1001\n",
      "epoch 13/10000 (batch 37000) - train loss: 2.1811, val loss: 1.9156\n",
      "epoch 14/10000 (batch 38000) - train loss: 1.8258, val loss: 1.7423\n",
      "epoch 14/10000 (batch 39000) - train loss: 1.7455, val loss: 1.5880\n",
      "epoch 15/10000 (batch 40000) - train loss: 1.4638, val loss: 1.4513\n",
      "epoch 15/10000 (batch 41000) - train loss: 1.4019, val loss: 1.3384\n",
      "epoch 15/10000 (batch 42000) - train loss: 1.3498, val loss: 1.2435\n",
      "epoch 16/10000 (batch 43000) - train loss: 1.1801, val loss: 1.1707\n",
      "epoch 16/10000 (batch 44000) - train loss: 1.1491, val loss: 1.1098\n",
      "epoch 16/10000 (batch 45000) - train loss: 1.1215, val loss: 1.0665\n",
      "epoch 17/10000 (batch 46000) - train loss: 1.0413, val loss: 1.0310\n",
      "epoch 17/10000 (batch 47000) - train loss: 1.0295, val loss: 1.0099\n",
      "epoch 17/10000 (batch 48000) - train loss: 1.0184, val loss: 0.9935\n",
      "epoch 18/10000 (batch 49000) - train loss: 0.9886, val loss: 0.9797\n",
      "epoch 18/10000 (batch 50000) - train loss: 0.9854, val loss: 0.9714\n",
      "epoch 18/10000 (batch 51000) - train loss: 0.9786, val loss: 0.9605\n",
      "epoch 19/10000 (batch 52000) - train loss: 0.9636, val loss: 0.9604\n",
      "epoch 19/10000 (batch 53000) - train loss: 0.9613, val loss: 0.9526\n",
      "epoch 19/10000 (batch 54000) - train loss: 0.9594, val loss: 0.9486\n",
      "epoch 20/10000 (batch 55000) - train loss: 0.9460, val loss: 0.9437\n",
      "epoch 20/10000 (batch 56000) - train loss: 0.9472, val loss: 0.9387\n",
      "epoch 20/10000 (batch 57000) - train loss: 0.9437, val loss: 0.9357\n",
      "epoch 21/10000 (batch 58000) - train loss: 0.9351, val loss: 0.9314\n",
      "epoch 21/10000 (batch 59000) - train loss: 0.9368, val loss: 0.9276\n",
      "epoch 22/10000 (batch 60000) - train loss: 0.9170, val loss: 0.9246\n",
      "epoch 22/10000 (batch 61000) - train loss: 0.9233, val loss: 0.9215\n",
      "epoch 22/10000 (batch 62000) - train loss: 0.9244, val loss: 0.9168\n",
      "epoch 23/10000 (batch 63000) - train loss: 0.9104, val loss: 0.9130\n",
      "epoch 23/10000 (batch 64000) - train loss: 0.9190, val loss: 0.9086\n",
      "epoch 23/10000 (batch 65000) - train loss: 0.9161, val loss: 0.9062\n",
      "epoch 24/10000 (batch 66000) - train loss: 0.8995, val loss: 0.9014\n",
      "epoch 24/10000 (batch 67000) - train loss: 0.9035, val loss: 0.8999\n",
      "epoch 24/10000 (batch 68000) - train loss: 0.9033, val loss: 0.8932\n",
      "epoch 25/10000 (batch 69000) - train loss: 0.8963, val loss: 0.8853\n",
      "epoch 25/10000 (batch 70000) - train loss: 0.8921, val loss: 0.8826\n",
      "epoch 25/10000 (batch 71000) - train loss: 0.8883, val loss: 0.8796\n",
      "epoch 26/10000 (batch 72000) - train loss: 0.8833, val loss: 0.8734\n",
      "epoch 26/10000 (batch 73000) - train loss: 0.8777, val loss: 0.8690\n",
      "epoch 26/10000 (batch 74000) - train loss: 0.8745, val loss: 0.8639\n",
      "epoch 27/10000 (batch 75000) - train loss: 0.8672, val loss: 0.8607\n",
      "epoch 27/10000 (batch 76000) - train loss: 0.8653, val loss: 0.8557\n",
      "epoch 28/10000 (batch 77000) - train loss: 0.8448, val loss: 0.8481\n",
      "epoch 28/10000 (batch 78000) - train loss: 0.8474, val loss: 0.8422\n",
      "epoch 28/10000 (batch 79000) - train loss: 0.8460, val loss: 0.8392\n",
      "epoch 29/10000 (batch 80000) - train loss: 0.8468, val loss: 0.8322\n",
      "epoch 29/10000 (batch 81000) - train loss: 0.8328, val loss: 0.8271\n",
      "epoch 29/10000 (batch 82000) - train loss: 0.8305, val loss: 0.8225\n",
      "epoch 30/10000 (batch 83000) - train loss: 0.8187, val loss: 0.8171\n",
      "epoch 30/10000 (batch 84000) - train loss: 0.8185, val loss: 0.8123\n",
      "epoch 30/10000 (batch 85000) - train loss: 0.8149, val loss: 0.8065\n",
      "epoch 31/10000 (batch 86000) - train loss: 0.8066, val loss: 0.8021\n",
      "epoch 31/10000 (batch 87000) - train loss: 0.8041, val loss: 0.7956\n",
      "epoch 31/10000 (batch 88000) - train loss: 0.8005, val loss: 0.7927\n",
      "epoch 32/10000 (batch 89000) - train loss: 0.7927, val loss: 0.7862\n",
      "epoch 32/10000 (batch 90000) - train loss: 0.7886, val loss: 0.7837\n",
      "epoch 32/10000 (batch 91000) - train loss: 0.7881, val loss: 0.7809\n",
      "epoch 33/10000 (batch 92000) - train loss: 0.7807, val loss: 0.7747\n",
      "epoch 33/10000 (batch 93000) - train loss: 0.7806, val loss: 0.7712\n",
      "epoch 33/10000 (batch 94000) - train loss: 0.7789, val loss: 0.7676\n",
      "epoch 34/10000 (batch 95000) - train loss: 0.7748, val loss: 0.7655\n",
      "epoch 34/10000 (batch 96000) - train loss: 0.7698, val loss: 0.7632\n",
      "epoch 35/10000 (batch 97000) - train loss: 0.7566, val loss: 0.7620\n",
      "epoch 35/10000 (batch 98000) - train loss: 0.7605, val loss: 0.7578\n",
      "epoch 35/10000 (batch 99000) - train loss: 0.7585, val loss: 0.7576\n",
      "epoch 36/10000 (batch 100000) - train loss: 0.7585, val loss: 0.7533\n",
      "epoch 36/10000 (batch 101000) - train loss: 0.7541, val loss: 0.7528\n",
      "epoch 36/10000 (batch 102000) - train loss: 0.7547, val loss: 0.7514\n",
      "epoch 37/10000 (batch 103000) - train loss: 0.7628, val loss: 0.7492\n",
      "epoch 37/10000 (batch 104000) - train loss: 0.7534, val loss: 0.7467\n",
      "epoch 37/10000 (batch 105000) - train loss: 0.7526, val loss: 0.7459\n",
      "epoch 38/10000 (batch 106000) - train loss: 0.7445, val loss: 0.7437\n",
      "epoch 38/10000 (batch 107000) - train loss: 0.7492, val loss: 0.7422\n",
      "epoch 38/10000 (batch 108000) - train loss: 0.7468, val loss: 0.7427\n",
      "Early stopping triggered at epoch 38, batch 108000\n",
      "embedding generation completed! :) \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e137f779887496787a503ad24adb5f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n",
      "/home/gokul/.local/lib/python3.10/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n",
      "fractions:   0%|          | 0/10 [00:00<?, ?it/s]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/home/gokul/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "Monitored metric elbo_validation did not improve in the last 45 records. Best score: 384.121. Signaling Trainer to stop.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "epoch 500/10000 (batch 1000) - train loss: 1.8886, val loss: 1.8901\n",
      "epoch 1000/10000 (batch 2000) - train loss: 1.0929, val loss: 1.2083\n",
      "epoch 1500/10000 (batch 3000) - train loss: 0.9918, val loss: 1.1868\n",
      "epoch 2000/10000 (batch 4000) - train loss: 0.9387, val loss: 1.1619\n",
      "epoch 2500/10000 (batch 5000) - train loss: 0.8676, val loss: 1.1475\n",
      "epoch 3000/10000 (batch 6000) - train loss: 0.8933, val loss: 1.0486\n",
      "epoch 3500/10000 (batch 7000) - train loss: 0.8927, val loss: 1.1049\n",
      "Early stopping triggered at epoch 3500, batch 7000\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/home/gokul/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "Monitored metric elbo_validation did not improve in the last 45 records. Best score: 439.181. Signaling Trainer to stop.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/home/gokul/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=400` reached.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/home/gokul/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "Monitored metric elbo_validation did not improve in the last 45 records. Best score: 323.443. Signaling Trainer to stop.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "epoch 36/10000 (batch 1000) - train loss: 6.9578, val loss: 6.9568\n",
      "epoch 72/10000 (batch 2000) - train loss: 5.8982, val loss: 5.8834\n",
      "epoch 108/10000 (batch 3000) - train loss: 4.1411, val loss: 4.1216\n",
      "epoch 143/10000 (batch 4000) - train loss: 2.0768, val loss: 2.0330\n",
      "epoch 179/10000 (batch 5000) - train loss: 1.2887, val loss: 1.2651\n",
      "epoch 215/10000 (batch 6000) - train loss: 1.1896, val loss: 1.1251\n",
      "epoch 250/10000 (batch 7000) - train loss: 1.1160, val loss: 1.1063\n",
      "epoch 286/10000 (batch 8000) - train loss: 1.0962, val loss: 1.0555\n",
      "epoch 322/10000 (batch 9000) - train loss: 1.0417, val loss: 1.0221\n",
      "epoch 358/10000 (batch 10000) - train loss: 0.9694, val loss: 0.9282\n",
      "epoch 393/10000 (batch 11000) - train loss: 0.9383, val loss: 0.9182\n",
      "epoch 429/10000 (batch 12000) - train loss: 0.9235, val loss: 0.8822\n",
      "epoch 465/10000 (batch 13000) - train loss: 0.9385, val loss: 0.8905\n",
      "Early stopping triggered at epoch 465, batch 13000\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/home/gokul/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=400` reached.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Monitored metric elbo_validation did not improve in the last 45 records. Best score: 346.559. Signaling Trainer to stop.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Monitored metric elbo_validation did not improve in the last 45 records. Best score: 321.056. Signaling Trainer to stop.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "epoch 4/10000 (batch 1000) - train loss: 7.3140, val loss: 7.3377\n",
      "epoch 8/10000 (batch 2000) - train loss: 7.2346, val loss: 7.2351\n",
      "epoch 11/10000 (batch 3000) - train loss: 7.0833, val loss: 7.0695\n",
      "epoch 15/10000 (batch 4000) - train loss: 6.8391, val loss: 6.8372\n",
      "epoch 18/10000 (batch 5000) - train loss: 6.5654, val loss: 6.5390\n",
      "epoch 22/10000 (batch 6000) - train loss: 6.1766, val loss: 6.1759\n",
      "epoch 25/10000 (batch 7000) - train loss: 5.7835, val loss: 5.7454\n",
      "epoch 29/10000 (batch 8000) - train loss: 5.2631, val loss: 5.2483\n",
      "epoch 32/10000 (batch 9000) - train loss: 4.7394, val loss: 4.6862\n",
      "epoch 36/10000 (batch 10000) - train loss: 4.0732, val loss: 4.0575\n",
      "epoch 39/10000 (batch 11000) - train loss: 3.4451, val loss: 3.3821\n",
      "epoch 43/10000 (batch 12000) - train loss: 2.7095, val loss: 2.6877\n",
      "epoch 46/10000 (batch 13000) - train loss: 2.1194, val loss: 2.0690\n",
      "epoch 50/10000 (batch 14000) - train loss: 1.6404, val loss: 1.6189\n",
      "epoch 53/10000 (batch 15000) - train loss: 1.3900, val loss: 1.3642\n",
      "epoch 57/10000 (batch 16000) - train loss: 1.2714, val loss: 1.2586\n",
      "epoch 60/10000 (batch 17000) - train loss: 1.2192, val loss: 1.1992\n",
      "epoch 64/10000 (batch 18000) - train loss: 1.2011, val loss: 1.1737\n",
      "epoch 67/10000 (batch 19000) - train loss: 1.1631, val loss: 1.1598\n",
      "epoch 71/10000 (batch 20000) - train loss: 1.1608, val loss: 1.1402\n",
      "epoch 74/10000 (batch 21000) - train loss: 1.1198, val loss: 1.1196\n",
      "epoch 78/10000 (batch 22000) - train loss: 1.1262, val loss: 1.0994\n",
      "epoch 81/10000 (batch 23000) - train loss: 1.0901, val loss: 1.0808\n",
      "epoch 85/10000 (batch 24000) - train loss: 1.0743, val loss: 1.0633\n",
      "epoch 88/10000 (batch 25000) - train loss: 1.0513, val loss: 1.0452\n",
      "epoch 92/10000 (batch 26000) - train loss: 1.0253, val loss: 1.0260\n",
      "epoch 95/10000 (batch 27000) - train loss: 1.0171, val loss: 1.0018\n",
      "epoch 99/10000 (batch 28000) - train loss: 0.9796, val loss: 0.9834\n",
      "epoch 102/10000 (batch 29000) - train loss: 0.9749, val loss: 0.9679\n",
      "epoch 106/10000 (batch 30000) - train loss: 0.9491, val loss: 0.9569\n",
      "epoch 109/10000 (batch 31000) - train loss: 0.9482, val loss: 0.9380\n",
      "epoch 113/10000 (batch 32000) - train loss: 0.9301, val loss: 0.9295\n",
      "epoch 116/10000 (batch 33000) - train loss: 0.9294, val loss: 0.9230\n",
      "epoch 120/10000 (batch 34000) - train loss: 0.9380, val loss: 0.9134\n",
      "epoch 123/10000 (batch 35000) - train loss: 0.9277, val loss: 0.9127\n",
      "epoch 127/10000 (batch 36000) - train loss: 0.9064, val loss: 0.9098\n",
      "epoch 130/10000 (batch 37000) - train loss: 0.9187, val loss: 0.9106\n",
      "Early stopping triggered at epoch 130, batch 37000\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "`Trainer.fit` stopped: `max_epochs=326` reached.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "`Trainer.fit` stopped: `max_epochs=151` reached.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "`Trainer.fit` stopped: `max_epochs=70` reached.\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData\n",
      "         setup                                                                  \n",
      "epoch 1/10000 (batch 1000) - train loss: 6.8017, val loss: 6.8042\n",
      "epoch 1/10000 (batch 2000) - train loss: 6.8005, val loss: 6.7939\n",
      "epoch 2/10000 (batch 3000) - train loss: 6.7789, val loss: 6.7769\n",
      "epoch 2/10000 (batch 4000) - train loss: 6.7675, val loss: 6.7533\n",
      "epoch 2/10000 (batch 5000) - train loss: 6.7543, val loss: 6.7235\n",
      "epoch 3/10000 (batch 6000) - train loss: 6.6923, val loss: 6.6869\n",
      "epoch 3/10000 (batch 7000) - train loss: 6.6719, val loss: 6.6434\n",
      "epoch 3/10000 (batch 8000) - train loss: 6.6490, val loss: 6.5939\n",
      "epoch 4/10000 (batch 9000) - train loss: 6.5505, val loss: 6.5376\n",
      "epoch 4/10000 (batch 10000) - train loss: 6.5199, val loss: 6.4737\n",
      "epoch 4/10000 (batch 11000) - train loss: 6.4871, val loss: 6.4044\n",
      "epoch 5/10000 (batch 12000) - train loss: 6.3514, val loss: 6.3278\n",
      "epoch 5/10000 (batch 13000) - train loss: 6.3110, val loss: 6.2448\n",
      "epoch 5/10000 (batch 14000) - train loss: 6.2686, val loss: 6.1552\n",
      "epoch 6/10000 (batch 15000) - train loss: 6.0962, val loss: 6.0588\n",
      "epoch 6/10000 (batch 16000) - train loss: 6.0460, val loss: 5.9561\n",
      "epoch 6/10000 (batch 17000) - train loss: 5.9935, val loss: 5.8460\n",
      "epoch 7/10000 (batch 18000) - train loss: 5.7832, val loss: 5.7306\n",
      "epoch 7/10000 (batch 19000) - train loss: 5.7235, val loss: 5.6075\n",
      "epoch 8/10000 (batch 20000) - train loss: 5.4804, val loss: 5.4781\n",
      "epoch 8/10000 (batch 21000) - train loss: 5.4146, val loss: 5.3421\n",
      "epoch 8/10000 (batch 22000) - train loss: 5.3450, val loss: 5.1995\n",
      "epoch 9/10000 (batch 23000) - train loss: 5.0656, val loss: 5.0504\n",
      "epoch 9/10000 (batch 24000) - train loss: 4.9905, val loss: 4.8943\n",
      "epoch 9/10000 (batch 25000) - train loss: 4.9096, val loss: 4.7318\n",
      "epoch 10/10000 (batch 26000) - train loss: 4.5934, val loss: 4.5635\n",
      "epoch 10/10000 (batch 27000) - train loss: 4.5075, val loss: 4.3879\n",
      "epoch 10/10000 (batch 28000) - train loss: 4.4183, val loss: 4.2054\n",
      "epoch 11/10000 (batch 29000) - train loss: 4.0669, val loss: 4.0179\n",
      "epoch 11/10000 (batch 30000) - train loss: 3.9716, val loss: 3.8259\n",
      "epoch 11/10000 (batch 31000) - train loss: 3.8741, val loss: 3.6268\n",
      "epoch 12/10000 (batch 32000) - train loss: 3.4895, val loss: 3.4241\n",
      "epoch 12/10000 (batch 33000) - train loss: 3.3905, val loss: 3.2180\n",
      "epoch 12/10000 (batch 34000) - train loss: 3.2872, val loss: 3.0115\n",
      "epoch 13/10000 (batch 35000) - train loss: 2.8899, val loss: 2.8038\n",
      "epoch 13/10000 (batch 36000) - train loss: 2.7846, val loss: 2.5988\n",
      "epoch 13/10000 (batch 37000) - train loss: 2.6819, val loss: 2.3991\n",
      "epoch 14/10000 (batch 38000) - train loss: 2.2952, val loss: 2.2047\n",
      "epoch 14/10000 (batch 39000) - train loss: 2.2055, val loss: 2.0253\n",
      "epoch 15/10000 (batch 40000) - train loss: 1.8683, val loss: 1.8647\n",
      "epoch 15/10000 (batch 41000) - train loss: 1.7994, val loss: 1.7145\n",
      "epoch 15/10000 (batch 42000) - train loss: 1.7291, val loss: 1.5910\n",
      "epoch 16/10000 (batch 43000) - train loss: 1.5057, val loss: 1.4870\n",
      "epoch 16/10000 (batch 44000) - train loss: 1.4606, val loss: 1.4031\n",
      "epoch 16/10000 (batch 45000) - train loss: 1.4219, val loss: 1.3407\n",
      "epoch 17/10000 (batch 46000) - train loss: 1.3137, val loss: 1.2901\n",
      "epoch 17/10000 (batch 47000) - train loss: 1.2884, val loss: 1.2597\n",
      "epoch 17/10000 (batch 48000) - train loss: 1.2701, val loss: 1.2294\n",
      "epoch 18/10000 (batch 49000) - train loss: 1.2184, val loss: 1.2145\n",
      "epoch 18/10000 (batch 50000) - train loss: 1.2130, val loss: 1.1979\n",
      "epoch 18/10000 (batch 51000) - train loss: 1.2085, val loss: 1.1910\n",
      "epoch 19/10000 (batch 52000) - train loss: 1.1864, val loss: 1.1811\n",
      "epoch 19/10000 (batch 53000) - train loss: 1.1827, val loss: 1.1711\n",
      "epoch 19/10000 (batch 54000) - train loss: 1.1803, val loss: 1.1650\n",
      "epoch 20/10000 (batch 55000) - train loss: 1.1629, val loss: 1.1600\n",
      "epoch 20/10000 (batch 56000) - train loss: 1.1639, val loss: 1.1510\n",
      "epoch 20/10000 (batch 57000) - train loss: 1.1614, val loss: 1.1453\n",
      "epoch 21/10000 (batch 58000) - train loss: 1.1534, val loss: 1.1427\n",
      "epoch 21/10000 (batch 59000) - train loss: 1.1468, val loss: 1.1386\n",
      "epoch 22/10000 (batch 60000) - train loss: 1.1417, val loss: 1.1341\n",
      "epoch 22/10000 (batch 61000) - train loss: 1.1306, val loss: 1.1241\n",
      "epoch 22/10000 (batch 62000) - train loss: 1.1301, val loss: 1.1201\n",
      "epoch 23/10000 (batch 63000) - train loss: 1.1110, val loss: 1.1150\n",
      "epoch 23/10000 (batch 64000) - train loss: 1.1129, val loss: 1.1108\n",
      "epoch 23/10000 (batch 65000) - train loss: 1.1106, val loss: 1.1022\n",
      "epoch 24/10000 (batch 66000) - train loss: 1.0998, val loss: 1.0982\n",
      "epoch 24/10000 (batch 67000) - train loss: 1.0985, val loss: 1.0904\n",
      "epoch 24/10000 (batch 68000) - train loss: 1.0948, val loss: 1.0853\n",
      "epoch 25/10000 (batch 69000) - train loss: 1.0857, val loss: 1.0743\n",
      "epoch 25/10000 (batch 70000) - train loss: 1.0816, val loss: 1.0703\n",
      "epoch 25/10000 (batch 71000) - train loss: 1.0780, val loss: 1.0643\n",
      "epoch 26/10000 (batch 72000) - train loss: 1.0631, val loss: 1.0578\n",
      "epoch 26/10000 (batch 73000) - train loss: 1.0608, val loss: 1.0501\n",
      "epoch 26/10000 (batch 74000) - train loss: 1.0574, val loss: 1.0417\n",
      "epoch 27/10000 (batch 75000) - train loss: 1.0442, val loss: 1.0352\n",
      "epoch 27/10000 (batch 76000) - train loss: 1.0409, val loss: 1.0308\n",
      "epoch 28/10000 (batch 77000) - train loss: 1.0113, val loss: 1.0215\n",
      "epoch 28/10000 (batch 78000) - train loss: 1.0212, val loss: 1.0181\n",
      "epoch 28/10000 (batch 79000) - train loss: 1.0199, val loss: 1.0109\n",
      "epoch 29/10000 (batch 80000) - train loss: 1.0073, val loss: 1.0017\n",
      "epoch 29/10000 (batch 81000) - train loss: 1.0082, val loss: 0.9960\n",
      "epoch 29/10000 (batch 82000) - train loss: 1.0006, val loss: 0.9910\n",
      "epoch 30/10000 (batch 83000) - train loss: 0.9908, val loss: 0.9851\n",
      "epoch 30/10000 (batch 84000) - train loss: 0.9880, val loss: 0.9791\n",
      "epoch 30/10000 (batch 85000) - train loss: 0.9850, val loss: 0.9741\n",
      "epoch 31/10000 (batch 86000) - train loss: 0.9706, val loss: 0.9696\n",
      "epoch 31/10000 (batch 87000) - train loss: 0.9697, val loss: 0.9633\n",
      "epoch 31/10000 (batch 88000) - train loss: 0.9690, val loss: 0.9601\n",
      "epoch 32/10000 (batch 89000) - train loss: 0.9667, val loss: 0.9561\n",
      "epoch 32/10000 (batch 90000) - train loss: 0.9600, val loss: 0.9503\n",
      "epoch 32/10000 (batch 91000) - train loss: 0.9579, val loss: 0.9455\n",
      "epoch 33/10000 (batch 92000) - train loss: 0.9541, val loss: 0.9434\n",
      "epoch 33/10000 (batch 93000) - train loss: 0.9496, val loss: 0.9403\n",
      "epoch 33/10000 (batch 94000) - train loss: 0.9459, val loss: 0.9386\n",
      "epoch 34/10000 (batch 95000) - train loss: 0.9363, val loss: 0.9353\n",
      "epoch 34/10000 (batch 96000) - train loss: 0.9404, val loss: 0.9326\n",
      "epoch 35/10000 (batch 97000) - train loss: 0.9436, val loss: 0.9297\n",
      "epoch 35/10000 (batch 98000) - train loss: 0.9338, val loss: 0.9299\n",
      "Early stopping triggered at epoch 35, batch 98000\n",
      "embedding generation completed! :) \n"
     ]
    }
   ],
   "source": [
    "for q in tqdm(qualities):\n",
    "    for r in tqdm(range(replicates), leave=False):\n",
    "        # with io.capture_output() as captured:\n",
    "        !python PBMC_CITEseq_embedding_generation.py {q} {r} 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
