{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gokul/.local/lib/python3.10/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n"
     ]
    }
   ],
   "source": [
    "import scanpy as sc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import gc\n",
    "import sys\n",
    "from latentmi import lmi\n",
    "from data_utils import *\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nanoTxformer.model import nanoTxformer\n",
    "from nanoTxformer.train import train_model, generalization_loss\n",
    "from nanoTxformer.utils import get_mean_pooled_embeddings\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {\n",
    "    \"MI\" : [],\n",
    "    \"Layers\" : [],\n",
    "    \"Embedding size\" : [],\n",
    "    \"Heads\" : [],\n",
    "    \"Val. loss\" : [],\n",
    "    \"Extract layer\" : []\n",
    "}\n",
    "\n",
    "ad = sc.read_h5ad('../../scaling_playground/data/PBMC_CITEseq_Q%.3f_rep%d.h5ad'\n",
    "                  %(1., 0))\n",
    "\n",
    "seen, held_out = split_ad(ad, frac=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28517\n",
      "epoch 2/10000 (batch 1000) - train loss: 7.4290, val loss: 7.4170\n",
      "epoch 3/10000 (batch 2000) - train loss: 7.0215, val loss: 6.8082\n",
      "epoch 5/10000 (batch 3000) - train loss: 5.8815, val loss: 5.7896\n",
      "epoch 6/10000 (batch 4000) - train loss: 4.7728, val loss: 4.4709\n",
      "epoch 8/10000 (batch 5000) - train loss: 3.1957, val loss: 3.1783\n",
      "epoch 9/10000 (batch 6000) - train loss: 2.3811, val loss: 2.2802\n",
      "epoch 10/10000 (batch 7000) - train loss: 1.9330, val loss: 1.8378\n",
      "epoch 12/10000 (batch 8000) - train loss: 1.6496, val loss: 1.6363\n",
      "epoch 13/10000 (batch 9000) - train loss: 1.5470, val loss: 1.5314\n",
      "epoch 15/10000 (batch 10000) - train loss: 1.4236, val loss: 1.4579\n",
      "epoch 16/10000 (batch 11000) - train loss: 1.3961, val loss: 1.3934\n",
      "epoch 17/10000 (batch 12000) - train loss: 1.3510, val loss: 1.3388\n",
      "epoch 19/10000 (batch 13000) - train loss: 1.3037, val loss: 1.2995\n",
      "epoch 20/10000 (batch 14000) - train loss: 1.2698, val loss: 1.2687\n",
      "epoch 22/10000 (batch 15000) - train loss: 1.2411, val loss: 1.2524\n",
      "epoch 23/10000 (batch 16000) - train loss: 1.2287, val loss: 1.2307\n",
      "epoch 24/10000 (batch 17000) - train loss: 1.2078, val loss: 1.2021\n",
      "epoch 26/10000 (batch 18000) - train loss: 1.1672, val loss: 1.1480\n",
      "epoch 27/10000 (batch 19000) - train loss: 0.9712, val loss: 0.9522\n",
      "epoch 29/10000 (batch 20000) - train loss: 0.9147, val loss: 0.9016\n",
      "epoch 30/10000 (batch 21000) - train loss: 0.8754, val loss: 0.8753\n",
      "epoch 31/10000 (batch 22000) - train loss: 0.8600, val loss: 0.8588\n",
      "epoch 33/10000 (batch 23000) - train loss: 0.8426, val loss: 0.8422\n",
      "epoch 34/10000 (batch 24000) - train loss: 0.8343, val loss: 0.8309\n",
      "epoch 36/10000 (batch 25000) - train loss: 0.8237, val loss: 0.8232\n",
      "epoch 37/10000 (batch 26000) - train loss: 0.8164, val loss: 0.8132\n",
      "epoch 38/10000 (batch 27000) - train loss: 0.8110, val loss: 0.8124\n",
      "epoch 40/10000 (batch 28000) - train loss: 0.8059, val loss: 0.8124\n",
      "Early stopping triggered at epoch 40, batch 28000\n",
      "epoch 299 (of max 300) ðŸŒ»ðŸŒ»ðŸŒ»ðŸŒ»ðŸŒ»ðŸŒ»ðŸŒ»ðŸŒ»ðŸŒ»{'MI': [2.8097649181157434, 2.768845131460193], 'Layers': [2, 2], 'Embedding size': [128, 128], 'Heads': [4, 4], 'Val. loss': [[7.417013059890669, 6.808228364142121, 5.789590504803658, 4.4709436003417276, 3.178315962989268, 2.280210724571766, 1.8377900865126886, 1.6363298056544953, 1.5313684139234152, 1.4579174001238209, 1.3934406466532565, 1.3387527540495954, 1.2994785695698476, 1.2687425130337096, 1.2523958796386068, 1.2306670852471877, 1.2020871760619223, 1.1480379955911353, 0.952197946052833, 0.9015626904957863, 0.8753400841283993, 0.858825119477802, 0.842201929775253, 0.8308812369767713, 0.8231699776107567, 0.8132200689965772, 0.8124072941257902, 0.8124481749711236], [7.417013059890669, 6.808228364142121, 5.789590504803658, 4.4709436003417276, 3.178315962989268, 2.280210724571766, 1.8377900865126886, 1.6363298056544953, 1.5313684139234152, 1.4579174001238209, 1.3934406466532565, 1.3387527540495954, 1.2994785695698476, 1.2687425130337096, 1.2523958796386068, 1.2306670852471877, 1.2020871760619223, 1.1480379955911353, 0.952197946052833, 0.9015626904957863, 0.8753400841283993, 0.858825119477802, 0.842201929775253, 0.8308812369767713, 0.8231699776107567, 0.8132200689965772, 0.8124072941257902, 0.8124481749711236]], 'Extract layer': ['Last', 'Second to last']}\n",
      "28517\n",
      "epoch 2/10000 (batch 1000) - train loss: 6.2615, val loss: 6.1034\n",
      "epoch 3/10000 (batch 2000) - train loss: 5.0327, val loss: 4.4812\n",
      "epoch 5/10000 (batch 3000) - train loss: 2.4244, val loss: 2.3068\n",
      "epoch 6/10000 (batch 4000) - train loss: 1.5232, val loss: 1.4503\n",
      "epoch 8/10000 (batch 5000) - train loss: 1.3155, val loss: 1.3321\n",
      "epoch 9/10000 (batch 6000) - train loss: 1.2968, val loss: 1.3065\n",
      "epoch 10/10000 (batch 7000) - train loss: 1.2754, val loss: 1.2762\n",
      "epoch 12/10000 (batch 8000) - train loss: 1.2429, val loss: 1.2570\n",
      "epoch 13/10000 (batch 9000) - train loss: 1.2170, val loss: 1.2324\n",
      "epoch 15/10000 (batch 10000) - train loss: 1.1830, val loss: 1.2108\n",
      "epoch 16/10000 (batch 11000) - train loss: 1.1837, val loss: 1.1923\n",
      "epoch 17/10000 (batch 12000) - train loss: 1.1040, val loss: 1.0516\n",
      "epoch 19/10000 (batch 13000) - train loss: 0.9342, val loss: 0.9386\n",
      "epoch 20/10000 (batch 14000) - train loss: 0.8633, val loss: 0.8674\n",
      "epoch 22/10000 (batch 15000) - train loss: 0.8218, val loss: 0.8440\n",
      "epoch 23/10000 (batch 16000) - train loss: 0.8318, val loss: 0.8354\n",
      "epoch 24/10000 (batch 17000) - train loss: 0.8164, val loss: 0.8282\n",
      "epoch 26/10000 (batch 18000) - train loss: 0.8111, val loss: 0.8195\n",
      "epoch 27/10000 (batch 19000) - train loss: 0.7982, val loss: 0.8049\n",
      "epoch 29/10000 (batch 20000) - train loss: 0.7951, val loss: 0.8033\n",
      "epoch 30/10000 (batch 21000) - train loss: 0.7900, val loss: 0.8017\n",
      "epoch 31/10000 (batch 22000) - train loss: 0.7910, val loss: 0.7953\n",
      "epoch 33/10000 (batch 23000) - train loss: 0.7778, val loss: 0.7951\n",
      "epoch 34/10000 (batch 24000) - train loss: 0.7758, val loss: 0.7860\n",
      "epoch 36/10000 (batch 25000) - train loss: 0.7725, val loss: 0.7827\n",
      "epoch 37/10000 (batch 26000) - train loss: 0.7721, val loss: 0.7832\n",
      "Early stopping triggered at epoch 37, batch 26000\n",
      "epoch 299 (of max 300) ðŸŒ»ðŸŒ»ðŸŒ»ðŸŒ»ðŸŒ»ðŸŒ»ðŸŒ»ðŸŒ»ðŸŒ»{'MI': [2.8097649181157434, 2.768845131460193, 3.025311936794756, 2.9509693337130734], 'Layers': [2, 2, 2, 2], 'Embedding size': [128, 128, 256, 256], 'Heads': [4, 4, 4, 4], 'Val. loss': [[7.417013059890669, 6.808228364142121, 5.789590504803658, 4.4709436003417276, 3.178315962989268, 2.280210724571766, 1.8377900865126886, 1.6363298056544953, 1.5313684139234152, 1.4579174001238209, 1.3934406466532565, 1.3387527540495954, 1.2994785695698476, 1.2687425130337096, 1.2523958796386068, 1.2306670852471877, 1.2020871760619223, 1.1480379955911353, 0.952197946052833, 0.9015626904957863, 0.8753400841283993, 0.858825119477802, 0.842201929775253, 0.8308812369767713, 0.8231699776107567, 0.8132200689965772, 0.8124072941257902, 0.8124481749711236], [7.417013059890669, 6.808228364142121, 5.789590504803658, 4.4709436003417276, 3.178315962989268, 2.280210724571766, 1.8377900865126886, 1.6363298056544953, 1.5313684139234152, 1.4579174001238209, 1.3934406466532565, 1.3387527540495954, 1.2994785695698476, 1.2687425130337096, 1.2523958796386068, 1.2306670852471877, 1.2020871760619223, 1.1480379955911353, 0.952197946052833, 0.9015626904957863, 0.8753400841283993, 0.858825119477802, 0.842201929775253, 0.8308812369767713, 0.8231699776107567, 0.8132200689965772, 0.8124072941257902, 0.8124481749711236], [6.103385214092396, 4.481169267012616, 2.306750237555038, 1.450266805360678, 1.332067693970861, 1.3064523976914781, 1.2762217850339594, 1.2570030012142024, 1.2324309087434024, 1.2107731253156928, 1.1923473561981175, 1.051617032774975, 0.9386019689144768, 0.8674364240368783, 0.8439898195185124, 0.8353620939415295, 0.8282428562651595, 0.8195377536377357, 0.8048896398329979, 0.8032631890130721, 0.8017177464539754, 0.79529793513653, 0.7950574499070736, 0.7860430170975915, 0.7827481675459333, 0.7831756040067207], [6.103385214092396, 4.481169267012616, 2.306750237555038, 1.450266805360678, 1.332067693970861, 1.3064523976914781, 1.2762217850339594, 1.2570030012142024, 1.2324309087434024, 1.2107731253156928, 1.1923473561981175, 1.051617032774975, 0.9386019689144768, 0.8674364240368783, 0.8439898195185124, 0.8353620939415295, 0.8282428562651595, 0.8195377536377357, 0.8048896398329979, 0.8032631890130721, 0.8017177464539754, 0.79529793513653, 0.7950574499070736, 0.7860430170975915, 0.7827481675459333, 0.7831756040067207]], 'Extract layer': ['Last', 'Second to last', 'Last', 'Second to last']}\n",
      "28517\n",
      "epoch 2/10000 (batch 1000) - train loss: 6.5539, val loss: 6.3986\n",
      "epoch 3/10000 (batch 2000) - train loss: 5.3333, val loss: 4.7696\n",
      "epoch 5/10000 (batch 3000) - train loss: 3.0537, val loss: 2.9574\n",
      "epoch 6/10000 (batch 4000) - train loss: 2.2186, val loss: 2.0783\n",
      "epoch 8/10000 (batch 5000) - train loss: 1.7418, val loss: 1.7417\n",
      "epoch 9/10000 (batch 6000) - train loss: 1.6422, val loss: 1.6211\n",
      "epoch 10/10000 (batch 7000) - train loss: 1.5698, val loss: 1.5514\n",
      "epoch 12/10000 (batch 8000) - train loss: 1.4789, val loss: 1.4826\n",
      "epoch 13/10000 (batch 9000) - train loss: 1.4457, val loss: 1.4259\n",
      "epoch 15/10000 (batch 10000) - train loss: 1.3949, val loss: 1.3704\n",
      "epoch 16/10000 (batch 11000) - train loss: 1.3346, val loss: 1.3230\n",
      "epoch 17/10000 (batch 12000) - train loss: 1.3070, val loss: 1.2876\n",
      "epoch 19/10000 (batch 13000) - train loss: 1.2664, val loss: 1.2625\n",
      "epoch 20/10000 (batch 14000) - train loss: 1.2402, val loss: 1.2253\n",
      "epoch 22/10000 (batch 15000) - train loss: 1.0111, val loss: 1.0045\n",
      "epoch 23/10000 (batch 16000) - train loss: 0.9557, val loss: 0.9419\n",
      "epoch 24/10000 (batch 17000) - train loss: 0.9205, val loss: 0.9051\n",
      "epoch 26/10000 (batch 18000) - train loss: 0.8863, val loss: 0.8792\n",
      "epoch 27/10000 (batch 19000) - train loss: 0.8688, val loss: 0.8676\n",
      "epoch 29/10000 (batch 20000) - train loss: 0.8315, val loss: 0.8455\n",
      "epoch 30/10000 (batch 21000) - train loss: 0.8456, val loss: 0.8419\n",
      "epoch 31/10000 (batch 22000) - train loss: 0.8358, val loss: 0.8235\n",
      "epoch 33/10000 (batch 23000) - train loss: 0.8290, val loss: 0.8241\n",
      "Early stopping triggered at epoch 33, batch 23000\n",
      "epoch 299 (of max 300) ðŸŒ»ðŸŒ»ðŸŒ»ðŸŒ»ðŸŒ»ðŸŒ»ðŸŒ»ðŸŒ»ðŸŒ»{'MI': [2.8097649181157434, 2.768845131460193, 3.025311936794756, 2.9509693337130734, 2.740928468644459, 2.7579357796809743], 'Layers': [2, 2, 2, 2, 6, 6], 'Embedding size': [128, 128, 256, 256, 128, 128], 'Heads': [4, 4, 4, 4, 4, 4], 'Val. loss': [[7.417013059890669, 6.808228364142121, 5.789590504803658, 4.4709436003417276, 3.178315962989268, 2.280210724571766, 1.8377900865126886, 1.6363298056544953, 1.5313684139234152, 1.4579174001238209, 1.3934406466532565, 1.3387527540495954, 1.2994785695698476, 1.2687425130337096, 1.2523958796386068, 1.2306670852471877, 1.2020871760619223, 1.1480379955911353, 0.952197946052833, 0.9015626904957863, 0.8753400841283993, 0.858825119477802, 0.842201929775253, 0.8308812369767713, 0.8231699776107567, 0.8132200689965772, 0.8124072941257902, 0.8124481749711236], [7.417013059890669, 6.808228364142121, 5.789590504803658, 4.4709436003417276, 3.178315962989268, 2.280210724571766, 1.8377900865126886, 1.6363298056544953, 1.5313684139234152, 1.4579174001238209, 1.3934406466532565, 1.3387527540495954, 1.2994785695698476, 1.2687425130337096, 1.2523958796386068, 1.2306670852471877, 1.2020871760619223, 1.1480379955911353, 0.952197946052833, 0.9015626904957863, 0.8753400841283993, 0.858825119477802, 0.842201929775253, 0.8308812369767713, 0.8231699776107567, 0.8132200689965772, 0.8124072941257902, 0.8124481749711236], [6.103385214092396, 4.481169267012616, 2.306750237555038, 1.450266805360678, 1.332067693970861, 1.3064523976914781, 1.2762217850339594, 1.2570030012142024, 1.2324309087434024, 1.2107731253156928, 1.1923473561981175, 1.051617032774975, 0.9386019689144768, 0.8674364240368783, 0.8439898195185124, 0.8353620939415295, 0.8282428562651595, 0.8195377536377357, 0.8048896398329979, 0.8032631890130721, 0.8017177464539754, 0.79529793513653, 0.7950574499070736, 0.7860430170975915, 0.7827481675459333, 0.7831756040067207], [6.103385214092396, 4.481169267012616, 2.306750237555038, 1.450266805360678, 1.332067693970861, 1.3064523976914781, 1.2762217850339594, 1.2570030012142024, 1.2324309087434024, 1.2107731253156928, 1.1923473561981175, 1.051617032774975, 0.9386019689144768, 0.8674364240368783, 0.8439898195185124, 0.8353620939415295, 0.8282428562651595, 0.8195377536377357, 0.8048896398329979, 0.8032631890130721, 0.8017177464539754, 0.79529793513653, 0.7950574499070736, 0.7860430170975915, 0.7827481675459333, 0.7831756040067207], [6.398618527217934, 4.769553996461871, 2.9573739562853163, 2.0782698192618176, 1.7417267662651514, 1.6211476810204655, 1.551357487293998, 1.4825513360167144, 1.4258561428233278, 1.3704177887423423, 1.32303561685421, 1.2876120865888476, 1.2624770821593592, 1.2252694319946065, 1.0045159561590586, 0.9418849449329746, 0.9050505757436362, 0.8792070352754717, 0.8675881636116863, 0.845474961565361, 0.8419353433381868, 0.8234649505752005, 0.8241010564166282], [6.398618527217934, 4.769553996461871, 2.9573739562853163, 2.0782698192618176, 1.7417267662651514, 1.6211476810204655, 1.551357487293998, 1.4825513360167144, 1.4258561428233278, 1.3704177887423423, 1.32303561685421, 1.2876120865888476, 1.2624770821593592, 1.2252694319946065, 1.0045159561590586, 0.9418849449329746, 0.9050505757436362, 0.8792070352754717, 0.8675881636116863, 0.845474961565361, 0.8419353433381868, 0.8234649505752005, 0.8241010564166282]], 'Extract layer': ['Last', 'Second to last', 'Last', 'Second to last', 'Last', 'Second to last']}\n",
      "28517\n",
      "epoch 2/10000 (batch 1000) - train loss: 4.8338, val loss: 4.4671\n",
      "epoch 3/10000 (batch 2000) - train loss: 2.5822, val loss: 1.9866\n",
      "epoch 5/10000 (batch 3000) - train loss: 1.3892, val loss: 1.3784\n",
      "epoch 6/10000 (batch 4000) - train loss: 1.3069, val loss: 1.3181\n",
      "epoch 8/10000 (batch 5000) - train loss: 1.2636, val loss: 1.2887\n",
      "epoch 9/10000 (batch 6000) - train loss: 1.2539, val loss: 1.2677\n",
      "epoch 10/10000 (batch 7000) - train loss: 1.2408, val loss: 1.2449\n",
      "epoch 12/10000 (batch 8000) - train loss: 1.2139, val loss: 1.2254\n",
      "epoch 13/10000 (batch 9000) - train loss: 1.1978, val loss: 1.2077\n",
      "epoch 15/10000 (batch 10000) - train loss: 0.9028, val loss: 0.9255\n",
      "epoch 16/10000 (batch 11000) - train loss: 0.8786, val loss: 0.8978\n",
      "epoch 17/10000 (batch 12000) - train loss: 0.8515, val loss: 0.8581\n",
      "epoch 19/10000 (batch 13000) - train loss: 0.8364, val loss: 0.8455\n",
      "epoch 20/10000 (batch 14000) - train loss: 0.8274, val loss: 0.8357\n",
      "epoch 22/10000 (batch 15000) - train loss: 0.8014, val loss: 0.8262\n",
      "epoch 23/10000 (batch 16000) - train loss: 0.8150, val loss: 0.8276\n",
      "Early stopping triggered at epoch 23, batch 16000\n",
      "epoch 299 (of max 300) ðŸŒ»ðŸŒ»ðŸŒ»ðŸŒ»ðŸŒ»ðŸŒ»ðŸŒ»ðŸŒ»ðŸŒ»{'MI': [2.8097649181157434, 2.768845131460193, 3.025311936794756, 2.9509693337130734, 2.740928468644459, 2.7579357796809743, 2.888020486523457, 3.020124102782615], 'Layers': [2, 2, 2, 2, 6, 6, 6, 6], 'Embedding size': [128, 128, 256, 256, 128, 128, 256, 256], 'Heads': [4, 4, 4, 4, 4, 4, 4, 4], 'Val. loss': [[7.417013059890669, 6.808228364142121, 5.789590504803658, 4.4709436003417276, 3.178315962989268, 2.280210724571766, 1.8377900865126886, 1.6363298056544953, 1.5313684139234152, 1.4579174001238209, 1.3934406466532565, 1.3387527540495954, 1.2994785695698476, 1.2687425130337096, 1.2523958796386068, 1.2306670852471877, 1.2020871760619223, 1.1480379955911353, 0.952197946052833, 0.9015626904957863, 0.8753400841283993, 0.858825119477802, 0.842201929775253, 0.8308812369767713, 0.8231699776107567, 0.8132200689965772, 0.8124072941257902, 0.8124481749711236], [7.417013059890669, 6.808228364142121, 5.789590504803658, 4.4709436003417276, 3.178315962989268, 2.280210724571766, 1.8377900865126886, 1.6363298056544953, 1.5313684139234152, 1.4579174001238209, 1.3934406466532565, 1.3387527540495954, 1.2994785695698476, 1.2687425130337096, 1.2523958796386068, 1.2306670852471877, 1.2020871760619223, 1.1480379955911353, 0.952197946052833, 0.9015626904957863, 0.8753400841283993, 0.858825119477802, 0.842201929775253, 0.8308812369767713, 0.8231699776107567, 0.8132200689965772, 0.8124072941257902, 0.8124481749711236], [6.103385214092396, 4.481169267012616, 2.306750237555038, 1.450266805360678, 1.332067693970861, 1.3064523976914781, 1.2762217850339594, 1.2570030012142024, 1.2324309087434024, 1.2107731253156928, 1.1923473561981175, 1.051617032774975, 0.9386019689144768, 0.8674364240368783, 0.8439898195185124, 0.8353620939415295, 0.8282428562651595, 0.8195377536377357, 0.8048896398329979, 0.8032631890130721, 0.8017177464539754, 0.79529793513653, 0.7950574499070736, 0.7860430170975915, 0.7827481675459333, 0.7831756040067207], [6.103385214092396, 4.481169267012616, 2.306750237555038, 1.450266805360678, 1.332067693970861, 1.3064523976914781, 1.2762217850339594, 1.2570030012142024, 1.2324309087434024, 1.2107731253156928, 1.1923473561981175, 1.051617032774975, 0.9386019689144768, 0.8674364240368783, 0.8439898195185124, 0.8353620939415295, 0.8282428562651595, 0.8195377536377357, 0.8048896398329979, 0.8032631890130721, 0.8017177464539754, 0.79529793513653, 0.7950574499070736, 0.7860430170975915, 0.7827481675459333, 0.7831756040067207], [6.398618527217934, 4.769553996461871, 2.9573739562853163, 2.0782698192618176, 1.7417267662651514, 1.6211476810204655, 1.551357487293998, 1.4825513360167144, 1.4258561428233278, 1.3704177887423423, 1.32303561685421, 1.2876120865888476, 1.2624770821593592, 1.2252694319946065, 1.0045159561590586, 0.9418849449329746, 0.9050505757436362, 0.8792070352754717, 0.8675881636116863, 0.845474961565361, 0.8419353433381868, 0.8234649505752005, 0.8241010564166282], [6.398618527217934, 4.769553996461871, 2.9573739562853163, 2.0782698192618176, 1.7417267662651514, 1.6211476810204655, 1.551357487293998, 1.4825513360167144, 1.4258561428233278, 1.3704177887423423, 1.32303561685421, 1.2876120865888476, 1.2624770821593592, 1.2252694319946065, 1.0045159561590586, 0.9418849449329746, 0.9050505757436362, 0.8792070352754717, 0.8675881636116863, 0.845474961565361, 0.8419353433381868, 0.8234649505752005, 0.8241010564166282], [4.467129809593456, 1.9866247340166714, 1.3784099046272205, 1.3180604533690978, 1.288666073719535, 1.2676569754455396, 1.2448539689311267, 1.2254417140272633, 1.207712588587153, 0.9254759063540207, 0.8978209075755234, 0.8580753027947952, 0.845515328771189, 0.8357392808214397, 0.8261520749953493, 0.827559386639017], [4.467129809593456, 1.9866247340166714, 1.3784099046272205, 1.3180604533690978, 1.288666073719535, 1.2676569754455396, 1.2448539689311267, 1.2254417140272633, 1.207712588587153, 0.9254759063540207, 0.8978209075755234, 0.8580753027947952, 0.845515328771189, 0.8357392808214397, 0.8261520749953493, 0.827559386639017]], 'Extract layer': ['Last', 'Second to last', 'Last', 'Second to last', 'Last', 'Second to last', 'Last', 'Second to last']}\n"
     ]
    }
   ],
   "source": [
    "layers = [2, 6]\n",
    "heads = [4]\n",
    "embs = [128, 256]\n",
    "for l in layers:\n",
    "    for h in heads:\n",
    "        for e in embs:\n",
    "\n",
    "            ad1, _ = split_ad(seen, frac=0.25)\n",
    "            print(len(ad1))\n",
    "\n",
    "            # batches = 10**4\n",
    "            # batch_size = 32\n",
    "            # epochs = batches * batch_size // len(seen)\n",
    "\n",
    "            # print(epochs)\n",
    "\n",
    "            model = nanoTxformer(ad1, embed_size=e, num_heads=h, num_encoder_layers=l).cuda()\n",
    "            \n",
    "            train_losses, val_losses = train_model(model, ad1, epochs=10**4)\n",
    "            \n",
    "            emb = get_mean_pooled_embeddings(model, held_out).cpu()\n",
    "            \n",
    "            emb_penultimate = get_mean_pooled_embeddings(model, held_out, layer_index=l-2).cpu()\n",
    "\n",
    "            pmis, _, _ = lmi.estimate(emb, held_out.obsm['protein_counts'], \n",
    "                                quiet=True, batch_size=2048)\n",
    "            \n",
    "            d[\"MI\"].append(np.nanmean(pmis))\n",
    "            d[\"Layers\"].append(l)\n",
    "            d[\"Embedding size\"].append(e)\n",
    "            d[\"Heads\"].append(h)\n",
    "            d[\"Val. loss\"].append(val_losses)\n",
    "            d['Extract layer'].append('Last')\n",
    "\n",
    "            pmis, _, _ = lmi.estimate(emb_penultimate, held_out.obsm['protein_counts'], \n",
    "                    quiet=True, batch_size=2048)\n",
    "            \n",
    "            d[\"MI\"].append(np.nanmean(pmis))\n",
    "            d[\"Layers\"].append(l)\n",
    "            d[\"Embedding size\"].append(e)\n",
    "            d[\"Heads\"].append(h)\n",
    "            d[\"Val. loss\"].append(val_losses)\n",
    "            d['Extract layer'].append('Second to last')\n",
    "\n",
    "            print(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(d).to_csv('../results/Tx_param_screen.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
